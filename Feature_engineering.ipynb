{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############this module is for feature engineering##############\n",
    "######usage: import Featu\n",
    "\n",
    "import pandas as pd\n",
    "teamname = 'emotional-support-vector-machine-unsw'\n",
    "data_folder='s3://tf-trachack-data/212/'\n",
    "root_folder='s3://tf-trachack-notebooks/'+teamname+'/jupyter/jovyan/'\n",
    "\n",
    "def create_new_data(f_type,name):\n",
    "    '''\n",
    "        Create new data set, should have created new_data folder in your own directory,\n",
    "        f_type: 'dev' or 'eval'\n",
    "    '''\n",
    "    print('Start to creating ....')\n",
    "    data_path = data_folder+\"data/\" + f_type + '/'\n",
    "    to_data_path = name + '/new_data/' + f_type + '_'\n",
    "    \n",
    "    upgrades=pd.read_csv(data_path + \"upgrades.csv\")\n",
    "    \n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    #customer_info\n",
    "    customer_info=pd.read_csv(data_path + \"customer_info.csv\")\n",
    "    customer_info['cus_used_days'] = pd.Series(pd.to_datetime(customer_info['redemption_date']) - pd.to_datetime(customer_info['first_activation_date'])).dt.days\n",
    "    customer_info['cus_used_days'] = scaler.fit_transform(customer_info['cus_used_days'].values.reshape(-1,1))\n",
    "    customer_info['cus_used_days'].fillna(-1,inplace = True)\n",
    "    customer_info['plan_name'].fillna(customer_info['plan_name'].mode()[0], inplace=True)\n",
    "\n",
    "    new_customer_info = pd.get_dummies(customer_info,columns=['carrier', 'plan_name'],drop_first=True)\n",
    "    select_features = ['line_id','cus_used_days', 'carrier_carrier 2', 'carrier_carrier 3','plan_name_plan 1', 'plan_name_plan 2', 'plan_name_plan 3','plan_name_plan 4']\n",
    "    new_customer_info = new_customer_info[select_features].drop_duplicates().reset_index(drop=True)\n",
    "    new_customer_info.to_csv(root_folder+ to_data_path + \"new_customer_info.csv\",header=True,index=None)\n",
    "\n",
    "    #phone_info\n",
    "    phone_info=pd.read_csv(data_path + \"phone_info.csv\")\n",
    "    phone_info['display_description'].fillna('is_miss',inplace=True)\n",
    "    type_list = ['HUAWEI H226C',  'Motorola XT2052DL', 'ZTE Z899VL',\n",
    "            'SAMSUNG S327VL', 'IPHONE 6', 'SAMSUNG G950U',\n",
    "           'IPHONE XR', 'SAMSUNG S260DL', 'LG L52VL', 'LG L322DL',\n",
    "           'SAMSUNG S727VL', 'IPHONE 7 PLUS', 'SAMSUNG S767VL',\n",
    "           'IPHONE 6S PLUS', 'HUAWEI H258C', 'Samsung S102DL',\n",
    "            'LG L555DL', 'IPHONE 7', \n",
    "            'IPHONE 5S', 'LM L413DL', 'ALCATEL A502DL',\n",
    "           'LG L58VL', 'Samsung S215DL', 'SAMSUNG S367VL', 'SAMSUNG S320VL',\n",
    "           'Samsung S111DL', 'KonnectOne K779HSDL', 'Motorola XT2005DL',\n",
    "           'iPhone SE 2', 'LG L455DL', 'SAMSUNG G970U1', 'ALCATEL A405DL',\n",
    "           'SAMSUNG S903VL', 'LG 441G', 'LG LML212VL', 'KonnectOne K500HPEL',\n",
    "           'MOTOROLA XT1925DL', 'LG L355DL', 'HUAWEI H710VL',\n",
    "           'SAMSUNG G930VL', 'LG L423DL', 'MOTOROLA XT1955DL', 'ZTE Z723EL',\n",
    "           'IPHONE SE', 'IPHONE 8', 'IPHONE 8 PLUS', 'SAMSUNG G960U1',\n",
    "           'LM L713DL', 'BLU B110DL', 'LG L164VL', 'ALCATEL A503DL',\n",
    "           'SAMSUNG S120VL', 'SAMSUNG S820L', 'LG LM414DL', 'LG L84VL',\n",
    "           'Samsung S515DL', 'Samsung S115DL', 'Samsung S205DL', 'LG L63BL',\n",
    "           'ZTE Z917VL', 'Motorola XT2041DL', 'LG L82VL', 'ZTE Z291DL',\n",
    "           'ZTE Z799VL', 'SAMSUNG G965U1', 'SAMSUNG S907VL', 'SAMSUNG S357BL',\n",
    "           'LG L158VL', 'LG L125DL', 'IPHONE XS', 'IPHONE 5C', 'ZTE Z233VL',\n",
    "           'ZTE Z963VL', 'LG 235C', 'ALCATEL A577VL', 'LG L44VL',\n",
    "           'SAMSUNG S975L', 'LG L61AL', 'LG L83BL', 'SAMSUNG G955U',\n",
    "           'LG L62VL', 'LG L64VL', 'MOTOROLA XT1920DL', 'LG231C', 'iPhone X',\n",
    "           'SAMSUNG S902L', 'LG 238C', 'SAMSUNG S550TL', 'LG L39C',\n",
    "           'ZTE Z289L', 'SAMSUNG S920L', 'ZTE Z837VL', 'LG 236C', 'LG 440G',\n",
    "           'ZTE Z558VL', 'LG L81AL', 'IPHONE 6 PLUS', 'iPhone 11 Pro Max',\n",
    "           'SAMSUNG S765C', 'iPhone 11', 'SAMSUNG S757BL', 'LG108C',\n",
    "           'SAMSUNG S380C', 'MOTOROLA XT1952DL', 'HUAWEI H883G',\n",
    "           'ALCATEL A501DL', 'IPHONE XS MAX', 'Samsung T528G', 'LG L43AL',\n",
    "           'ALCATEL A450TL', 'ZTE Z795G', 'LG L16C', 'iPhone 12',\n",
    "           'SAMSUNG S906L', 'BRING YOUR TABLET', 'FRANKLIN WIRELESS F900HSVL',\n",
    "           'SAMSUNG S336C', 'iPhone 11 Pro', 'ZTE Z716BL', 'LG LML211BL',\n",
    "           'LG L57BL', 'LG220C', 'LG L21G', 'LG L33L', 'LG L163BL',\n",
    "           'SAMSUNG S730G', 'MOTOROLA INC', 'LG L53BL', 'ZTE Z936L',\n",
    "           'Samsung R451C', 'ZTE Z288L', 'ALCATEL A574BL', 'ALCATEL A621BL',\n",
    "           'ZTE Z557BL', 'ALCATEL A564C', 'ALCATEL A521L', 'LG L35G',\n",
    "           'LG L22C', 'LG L157BL', 'Samsung N981U1', 'BLU B100DL', 'LG 306G',\n",
    "           'SAMSUNG S890L', 'RELIANCE AX54NC', 'SAMSUNG S968C',\n",
    "           'HUAWEI H210C', 'LG 221C', 'HUAWEI H892L', 'LG L51AL', 'ZTE Z796C',\n",
    "           'ZTE Z930L', 'ZTE Z719DL', 'ALCATEL A571VL', 'ALCATEL A392G',\n",
    "           'Alcatel A508DL', 'LG 442BG', 'HUAWEI H715BL', 'ZTE Z986DL',\n",
    "           'LG L31L', 'ZTE Z932L', 'ZTE Z916BL', 'Samsung G981U1',\n",
    "           'NOKIA E5G', 'LG L59BL', 'SAMSUNG G973U1', 'Samsung G770U1',\n",
    "           'SAMSUNG G975U1', 'LG L15G', 'LG L86C', 'LG 237C',\n",
    "           'MOTOROLA W419G', 'iPhone 12 Pro Max', 'Samsung N975U1',\n",
    "           'ZTE Z791G', 'ALCATEL A462C']\n",
    "\n",
    "    phone_info['fm_radio'].fillna('is_miss',inplace = True)\n",
    "    phone_info['available_online'].fillna('is_miss',inplace=True)\n",
    "    phone_info['device_type'].fillna('is_miss',inplace=True)\n",
    "    phone_info['device_type'].replace(['M2M','BYOT','MOBILE_BROADBAND','FEATURE_PHONE','WIRELESS_HOME_PHONE'], ['Others','Others','Others','Others','Others'], inplace=True)\n",
    "    phone_info['display_description'].replace(type_list,['Others']*len(type_list),inplace=True)\n",
    "    phone_info['data_capable'].fillna(0.0,inplace=True)\n",
    "    phone_info['device_lock_state'].fillna('is_miss',inplace=True)\n",
    "    phone_info['device_lock_state'].replace(['LOCKED','UNLOCKED'], ['Others','Others'], inplace=True)\n",
    "    phone_info['bluetooth'].fillna('is_miss',inplace = True)\n",
    "    phone_info['battery_removable'].fillna('is_miss',inplace = True)\n",
    "\n",
    "    new_phone_info = pd.get_dummies(phone_info,columns=['available_online','device_type','device_lock_state','data_capable','bluetooth','battery_removable','fm_radio','display_description'],drop_first=True)\n",
    "    select_features = ['line_id', 'available_online_Y', 'available_online_is_miss',\n",
    "           'device_type_Others', 'device_type_SMARTPHONE', 'device_type_is_miss',\n",
    "           'device_lock_state_UNLOCKABLE', 'device_lock_state_is_miss',\n",
    "           'data_capable_1.0', 'bluetooth_Y', 'bluetooth_is_miss',\n",
    "           'battery_removable_Y', 'battery_removable_is_miss', 'fm_radio_Y',\n",
    "           'fm_radio_is_miss', 'display_description_IPHONE 6S',\n",
    "           'display_description_LG L722DL', 'display_description_Others',\n",
    "           'display_description_Samsung S506DL', 'display_description_is_miss']\n",
    "    new_phone_info = new_phone_info[select_features].drop_duplicates().reset_index(drop=True)\n",
    "    new_phone_info.to_csv(root_folder + to_data_path + \"new_phone_info.csv\",header=True,index=None)\n",
    "\n",
    "    #redemptions\n",
    "    \n",
    "    redemptions=pd.read_csv(data_path + \"redemptions.csv\")\n",
    "\n",
    "    redemptions['red_count'] = scaler.fit_transform(redemptions.groupby('line_id')['channel'].transform('count').values.reshape(-1,1))#how ofen use\n",
    "    redemptions['red_mean_rev'] = scaler.fit_transform(redemptions.groupby('line_id')['gross_revenue'].transform('mean').values.reshape(-1,1))#how much\n",
    "    redemptions['channel_unique'] = scaler.fit_transform(redemptions.groupby('line_id')['channel'].transform('nunique').values.reshape(-1,1))#what kinds of channel\n",
    "    redemptions['red_type_unique'] = scaler.fit_transform(redemptions.groupby('line_id')['redemption_type'].transform('nunique').values.reshape(-1,1))# what kinds of paid type\n",
    "    redemptions['red_type_most_fre'] = redemptions['line_id'].map(redemptions.groupby('line_id')['redemption_type'].agg(lambda x: x.value_counts().idxmax()))#most frequently paid type\n",
    "    redemptions['channel_most_fre'] = redemptions['line_id'].map(redemptions.groupby('line_id')['channel'].agg(lambda x: x.value_counts().idxmax()))#most frequently channel\n",
    "    redemptions=pd.get_dummies(redemptions,columns=['red_type_most_fre','channel_most_fre'],drop_first=True)\n",
    "    redemptions = pd.merge(upgrades,redemptions,how='left',on='line_id')\n",
    "    lst = ['red_count','red_mean_rev','channel_unique','red_type_unique']\n",
    "    redemptions[lst] = redemptions[lst].fillna(redemptions[lst].mean())\n",
    "    redemptions.fillna(0,inplace=True)\n",
    "    \n",
    "    select_features = [e for e in redemptions.columns if e not in ['channel','gross_revenue','redemption_date','redemption_type','revenue_type']]\n",
    "    new_redemptions = redemptions[select_features].drop_duplicates().reset_index(drop=True)\n",
    "    new_redemptions.to_csv(root_folder+ to_data_path + \"new_redemptions.csv\",header=True,index=None)\n",
    "\n",
    "    #deactivations\n",
    "    deactivations=pd.read_csv(data_path + \"deactivations.csv\")\n",
    "\n",
    "    deactivations['dea_times'] = scaler.fit_transform(deactivations.groupby('line_id')['deactivation_date'].transform('count').values.reshape(-1,1))\n",
    "    deactivations['dea_reason_uni_counts'] = scaler.fit_transform(deactivations.groupby('line_id')['deactivation_reason'].transform('nunique').values.reshape(-1,1))\n",
    "    deactivations['dea_most_fre_reason'] = deactivations['line_id'].map(deactivations.groupby('line_id')['deactivation_reason'].agg(lambda x: x.value_counts().idxmax()))\n",
    "\n",
    "    temp_list = ['STOLEN','REMOVED_FROM_GROUP','MINCHANGE','STOLEN CREDIT CARD','DEVICE CHANGE INQUIRY','PORTED NO A/I','WN-SYSTEM ISSUED']\n",
    "    deactivations['dea_most_fre_reason'].replace(temp_list,['Other']*len(temp_list),inplace=True)\n",
    "    deactivations=pd.get_dummies(deactivations,columns=['dea_most_fre_reason'])#not useing drop first for merging the upgrade line_id and fillna with 0\n",
    "\n",
    "    new_deactivation = pd.merge(upgrades,deactivations,how='left',on = 'line_id')\n",
    "    new_deactivation['dea_times'].fillna(-1,inplace=True)#have no deactivation record\n",
    "    new_deactivation.fillna(0,inplace=True)#have no deactivation record\n",
    "\n",
    "    select_features = ['line_id', 'dea_times',\n",
    "           'dea_reason_uni_counts', 'dea_most_fre_reason_Other',\n",
    "           'dea_most_fre_reason_PASTDUE', 'dea_most_fre_reason_PORT OUT',\n",
    "           'dea_most_fre_reason_RISK ASSESSMENT', 'dea_most_fre_reason_UPGRADE','dea_most_fre_reason_CUSTOMER REQD']\n",
    "    new_deactivation = new_deactivation[select_features].drop_duplicates().reset_index(drop=True)\n",
    "    new_deactivation.to_csv(root_folder+ to_data_path+ \"new_deactivation.csv\",header=True,index=None)\n",
    "\n",
    "    #suspensions\n",
    "    suspensions=pd.read_csv(data_path + \"suspensions.csv\")\n",
    "    suspensions['sus_count'] = scaler.fit_transform(suspensions.groupby('line_id')['suspension_start_date'].transform('count').values.reshape(-1,1))\n",
    "    new_suspension = pd.merge(upgrades,suspensions,how='left',on = 'line_id')\n",
    "    new_suspension['sus_count'].fillna(-1,inplace=True)\n",
    "    select_features = ['line_id','sus_count']\n",
    "    new_suspension = new_suspension[select_features].drop_duplicates().reset_index(drop=True)\n",
    "    new_suspension.to_csv(root_folder+ to_data_path +\"new_suspension.csv\",header=True,index=None)\n",
    "\n",
    "    #network_usage\n",
    "    network_usage_domestic=pd.read_csv(data_path + \"network_usage_domestic.csv\")\n",
    "\n",
    "    network_usage_domestic['network_used_day'] = network_usage_domestic.groupby('line_id')['date'].transform('count')\n",
    "\n",
    "    features = ['line_id', 'hotspot_kb', 'mms_in', 'mms_out', 'sms_in',\n",
    "           'sms_out', 'total_kb', 'voice_count_in', 'voice_count_total',\n",
    "           'voice_min_in', 'voice_min_out']\n",
    "    new_features = ['mean_hotspot_kb', 'mean_mms_in', 'mean_mms_out', 'mean_sms_in',\n",
    "           'mean_sms_out', 'mean_total_kb', 'mean_voice_count_in', 'mean_voice_count_total',\n",
    "           'mean_voice_min_in', 'mean_voice_min_out']\n",
    "\n",
    "    temp = network_usage_domestic[features].groupby('line_id').transform('mean')\n",
    "    temp.columns = new_features\n",
    "\n",
    "    new_network_usage_domestic = pd.concat((network_usage_domestic,temp),axis=1)\n",
    "\n",
    "    new_features = ['network_used_day','mean_hotspot_kb', 'mean_mms_in', 'mean_mms_out', 'mean_sms_in',\n",
    "           'mean_sms_out', 'mean_total_kb', 'mean_voice_count_in', 'mean_voice_count_total',\n",
    "           'mean_voice_min_in', 'mean_voice_min_out']\n",
    "\n",
    "    new_network_usage_domestic[new_features] = scaler.fit_transform(new_network_usage_domestic[new_features])\n",
    "\n",
    "    #merge upgrade id_line and fill with mean\n",
    "    new_network_usage_domestic = pd.merge(upgrades,new_network_usage_domestic,how='left',on='line_id')\n",
    "    features = ['network_used_day','mean_hotspot_kb', 'mean_mms_in', 'mean_mms_out', 'mean_sms_in',\n",
    "           'mean_sms_out', 'mean_total_kb', 'mean_voice_count_in', 'mean_voice_count_total',\n",
    "           'mean_voice_min_in', 'mean_voice_min_out']\n",
    "    new_network_usage_domestic[features] = new_network_usage_domestic[features].fillna((new_network_usage_domestic[features].mean()))\n",
    "\n",
    "    #populate table\n",
    "    select_features = ['line_id','network_used_day','mean_hotspot_kb', 'mean_mms_in', 'mean_mms_out', 'mean_sms_in',\n",
    "           'mean_sms_out', 'mean_total_kb', 'mean_voice_count_in', 'mean_voice_count_total',\n",
    "           'mean_voice_min_in', 'mean_voice_min_out']\n",
    "    new_network_usage_domestic = new_network_usage_domestic[select_features].drop_duplicates().reset_index(drop=True)\n",
    "    new_network_usage_domestic.to_csv(root_folder+ to_data_path + \"new_networ_usage_domestic.csv\",header=True,index=None)\n",
    "    print('Finished.')\n",
    "    \n",
    "#merge all tables\n",
    "def merge_tables(f_type,name):\n",
    "    \"\"\"\n",
    "    merge the tables, f_type: 'dev' or 'eval'\n",
    "    must create new_data folder under your working folder\n",
    "    \"\"\"\n",
    "    print('Start to merge...')\n",
    "    data_path = name + '/new_data/' + f_type + '_'\n",
    "\n",
    "    new_redemptions = pd.read_csv(root_folder+ data_path + \"new_redemptions.csv\")\n",
    "    new_phone_info = pd.read_csv(root_folder+ data_path + \"new_phone_info.csv\")\n",
    "    new_customer_info = pd.read_csv(root_folder+ data_path +\"new_customer_info.csv\")\n",
    "    new_deactivation = pd.read_csv(root_folder+data_path + \"new_deactivation.csv\")\n",
    "    new_suspension = pd.read_csv(root_folder+ data_path +\"new_suspension.csv\")\n",
    "    new_network_usage_domestic = pd.read_csv(root_folder+ data_path +\"new_networ_usage_domestic.csv\")\n",
    "    upgrades=pd.read_csv(data_folder+\"data/\" + f_type + \"/upgrades.csv\")\n",
    "    upgrades = upgrades[['line_id']]\n",
    "\n",
    "    table_list = [new_redemptions,new_phone_info,new_customer_info,new_deactivation,new_suspension,new_network_usage_domestic,upgrades]\n",
    "    final_merge = pd.concat(table_list, join='inner', axis=1)\n",
    "    final_merge = final_merge.loc[:,~final_merge.columns.duplicated()]\n",
    "    final_merge.to_csv(root_folder + data_path + \"final_merge.csv\",header=True,index=None)\n",
    "    print('Finished')\n",
    "\n",
    "#call above functions\n",
    "def main(f_type,name):\n",
    "    create_new_data(f_type,name)\n",
    "    merge_tables(f_type,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "teamname = 'emotional-support-vector-machine-unsw'\n",
    "root_folder='s3://tf-trachack-notebooks/'+teamname+'/jupyter/jovyan/'\n",
    "\n",
    "data_train = pd.read_csv(root_folder+\"guohuan-li/new_data/dev_final_merge.csv\")\n",
    "data_val = pd.read_csv(root_folder+\"guohuan-li/new_data/eval_final_merge.csv\")\n",
    "\n",
    "#drop some features not in both datasets\n",
    "train_lst = list(data_train.columns[3:])\n",
    "val_lst = list(data_val.columns[1:])\n",
    "drop_lst = np.setdiff1d(train_lst,val_lst)\n",
    "data_train.drop(drop_lst, axis=1,inplace=True)\n",
    "train_lst = list(data_train.columns[3:])\n",
    "val_lst = list(data_val.columns[1:])\n",
    "drop_lst = np.setdiff1d(val_lst,train_lst)\n",
    "data_val.drop(drop_lst, axis=1,inplace=True)\n",
    "\n",
    "#extract the training data\n",
    "data_y = data_train['upgrade'].replace({'no':0,'yes':1})\n",
    "data_X = data_train.drop(['line_id','upgrade_date','upgrade'],axis = 1)\n",
    "data_val_X = data_val.drop(['line_id'],axis = 1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "DTC = DecisionTreeClassifier()\n",
    "RFC = RandomForestClassifier()\n",
    "ABC = AdaBoostClassifier()\n",
    "LR = LogisticRegression(max_iter=500)\n",
    "MLP = MLPClassifier(max_iter = 500)\n",
    "SVM = SVC()\n",
    "\n",
    "clfs = [DTC,RFC,ABC,LR,MLP,SVM]\n",
    "names = ['DTC','RFC','ABC','LR','MLP','SVM']\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "scoring = ['f1','precision','recall','accuracy']\n",
    "\n",
    "for i in range(len(names)):\n",
    "    scores  = cross_validate(clfs[i],data_X,data_y,cv = 10,scoring = scoring,return_train_score=True)\n",
    "    print(f'The model {names[i]} f1 is {scores[\"test_f1\"].mean()}, accu is {scores[\"test_accuracy\"].mean()}')\n",
    "    print()\n",
    "\n",
    "#RFC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'bootstrap': [True, False],\n",
    " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    " 'max_features': ['auto', 'sqrt'],\n",
    " 'min_samples_leaf': [1, 2, 4],\n",
    " 'min_samples_split': [2, 5, 10],\n",
    " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000],\n",
    " 'criterion' :['gini', 'entropy']}\n",
    "CV_rfc = GridSearchCV(estimator=RFC, param_grid=param_grid, cv= 10,n_jobs=-1,scoring = 'f1')\n",
    "CV_rfc.fit(data_X,data_y)\n",
    "CV_rfc.best_params_\n",
    "\n",
    "#Adaboost\n",
    "param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n",
    "              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n",
    "              \"n_estimators\": [10, 50, 100, 500],\n",
    "              'learning_rate' : [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "             }\n",
    "CV_abc = GridSearchCV(estimator=ABC, param_grid=param_grid, cv= 10,n_jobs=-1,scoring = 'f1')\n",
    "CV_abc.fit(data_X,data_y)\n",
    "CV_abc.best_params_\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
