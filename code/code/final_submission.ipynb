{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains the code in our final submission. We use the same feature extraction techniques to generate our data, and then run LGBM in order to classify our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==1.0.3 in /opt/conda/lib/python3.7/site-packages (1.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas==1.0.3) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas==1.0.3) (2020.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from pandas==1.0.3) (1.19.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas==1.0.3) (1.14.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# installing 1.0.3 because this version of pandas supports write to s3\n",
    "!pip install pandas==1.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "teamname = 'emotional-support-vector-machine-unsw'\n",
    "data_folder='s3://tf-trachack-data/212/'\n",
    "root_folder='s3://tf-trachack-notebooks/'+teamname+'/jupyter/jovyan/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache(obj, filepath):\n",
    "    \"\"\"Cache the result\"\"\"\n",
    "    directory = os.path.dirname(filepath)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    with open(filepath, 'wb+') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def is_cached(filepath):\n",
    "    \"\"\"Checks if the pickle file exists\"\"\"\n",
    "    return os.path.isfile(filepath)\n",
    "\n",
    "\n",
    "def load_cache(filepath):\n",
    "    \"\"\"Load the cached result\"\"\"\n",
    "    assert is_cached(filepath)\n",
    "    \n",
    "    with open(filepath, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_scale(series):\n",
    "    scaler = MinMaxScaler()\n",
    "    return scaler.fit_transform(series.values.reshape(-1,1))\n",
    "\n",
    "def standard_scale(series):\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(series.values.reshape(-1,1))\n",
    "\n",
    "def robust_scale(series):\n",
    "    scaler = RobustScaler()\n",
    "    return scaler.fit_transform(series.values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_datetime(series):\n",
    "    return pd.to_datetime(series, format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lte_downlink = {0:1,1:10,2:50,3:100,4:150,5:300,6:300,7:300,8:3000,9:450,10:450,11:600,12:600,13:390,14:3900,15:800,16:1050,17:25000,18:1200,19:1600,20:2000,21:1400}\n",
    "lte_uplink = {0:1,1:5,2:25,3:50,4:50,5:75,6:50,7:100,8:1500,9:50,10:100,11:50,12:100,13:150,14:1500,15:220,16:100,17:2100,18:210,19:13500,20:315,21:300}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Upgrades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract1(upgrades, filepath=None, overwrite=False):\n",
    "    upgrades = upgrades.drop('date_observed', axis=1)\n",
    "    df1 = upgrades.replace({'no':0,'yes':1})\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Customer Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract2(customer_info, filepath=None, overwrite=False):\n",
    "    customer_info = customer_info.copy()\n",
    "    \n",
    "    df2 = customer_info.copy()\n",
    "    df2 = pd.concat([df2, pd.get_dummies(df2['carrier'], prefix='carrier')], axis=1)\n",
    "\n",
    "    date1 = pd.to_datetime(df2['first_activation_date'], format=\"%Y-%m-%d\")\n",
    "    date2 = pd.to_datetime(df2['redemption_date'], format=\"%Y-%m-%d\")\n",
    "    df2[\"days_to_redemption\"] = np.log((date2 - date1).dt.days+1)\n",
    "    df2['days_to_redemption'].fillna((df2['days_to_redemption'].median()), inplace=True)    \n",
    "    scaler = MinMaxScaler()\n",
    "    df2['days_to_redemption'] = scaler.fit_transform(df2['days_to_redemption'].values.reshape(-1,1))\n",
    "\n",
    "    df2 = pd.concat([\n",
    "        df2,\n",
    "        pd.get_dummies(df2['plan_name'], prefix='plan')\n",
    "    ], axis=1)\n",
    "\n",
    "    df2.drop(\n",
    "        ['carrier', 'first_activation_date', 'plan_subtype', 'redemption_date', 'plan_name'], \n",
    "        axis=1, inplace=True\n",
    "    )\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Phone Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_number(s):\n",
    "    return int(re.search(r'\\d+', str(s)).group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_external_storage_capacity(s):\n",
    "    s = str(s)\n",
    "    if s == 'nan':\n",
    "        return np.nan\n",
    "    if '.' in s:\n",
    "        return float(s)\n",
    "    capacities = np.array(list(map(float, s.split('/'))))\n",
    "    capacity = np.median(capacities)\n",
    "    return capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def extract3(phone_info, upgrades, filepath=None, overwrite=False):\n",
    "    if is_cached(filepath) and not overwrite:\n",
    "        return load_cache(filepath)\n",
    "    \n",
    "    phone_info = phone_info.copy()\n",
    "    \n",
    "    # get has_phone\n",
    "    df3 = phone_info.copy()\n",
    "    df3['has_phone'] = np.where(df3['cpu_cores'].isnull(), 0, 1)\n",
    "\n",
    "    # get total cpus\n",
    "    df3['cpu_cores'] = df3['cpu_cores'].dropna().apply(str).apply(eval)\n",
    "    df3['cpu_cores'].fillna(df3['cpu_cores'].median(), inplace=True)\n",
    "    scaler = MinMaxScaler()\n",
    "    df3['cpu_cores'] = scaler.fit_transform(df3['cpu_cores'].values.reshape(-1,1))\n",
    "    \n",
    "    # expandable storage\n",
    "    df3['expandable_storage'].fillna(0, inplace=True)\n",
    "    \n",
    "    # remove uncommon device types\n",
    "    df3['gsma_device_type'].replace('WLAN Router', 'Other', inplace=True)\n",
    "    df3['gsma_device_type'].replace('Tablet', 'Other', inplace=True)\n",
    "    df3['gsma_device_type'].replace('Modem', 'Other', inplace=True)\n",
    "    df3 = pd.concat([df3, pd.get_dummies(df3['gsma_device_type'], prefix='device_type')], axis=1)\n",
    "    df3.drop(['gsma_device_type', 'device_type_Other'], axis=1, inplace=True)\n",
    "    \n",
    "    df3.drop(['gsma_model_name'], axis=1, inplace=True)\n",
    "    \n",
    "    # operating system\n",
    "    df3['gsma_operating_system'].loc[\n",
    "        (df3['gsma_operating_system'] != 'Android') &\n",
    "        (df3['gsma_operating_system'] != 'iOS') &\n",
    "        (df3['gsma_operating_system'] != 'Linux')\n",
    "    ] = 'Other'\n",
    "    df3 = pd.concat([df3, pd.get_dummies(df3['gsma_operating_system'], prefix='os')], axis=1)\n",
    "    df3.drop(\n",
    "        ['gsma_operating_system', 'os_family', 'os_name', 'os_vendor', 'os_version', \n",
    "         'manufacturer', 'os_Other'], axis=1, inplace=True)\n",
    "    \n",
    "    # internal stroage capacity, use medians\n",
    "    df3['internal_storage_capacity'] = df3['internal_storage_capacity'].apply(split_external_storage_capacity)\n",
    "    df3['internal_storage_capacity'].fillna(df3['internal_storage_capacity'].median(), inplace=True)\n",
    "    df3['internal_storage_capacity'] = np.log2(df3['internal_storage_capacity'])\n",
    "    scaler = StandardScaler()\n",
    "    df3['internal_storage_capacity'] = scaler.fit_transform(df3['internal_storage_capacity'].values.reshape(-1,1))\n",
    "    \n",
    "    df3['lte'].fillna(0, inplace=True)\n",
    "    df3['lte_advanced'].fillna(0, inplace=True)\n",
    "    \n",
    "    df3['lte_category'].fillna(df3['lte_category'].dropna().mode()[0], inplace=True)\n",
    "    df3['lte_downlink'] = df3['lte_category'].map(lte_downlink)\n",
    "    df3['lte_uplink'] = df3['lte_category'].map(lte_uplink)\n",
    "    scaler = RobustScaler()\n",
    "    df3['lte_downlink'] = scaler.fit_transform(df3['lte_downlink'].values.reshape(-1,1))\n",
    "    df3['lte_uplink'] = scaler.fit_transform(df3['lte_uplink'].values.reshape(-1,1))\n",
    "    df3['lte_downlink'] /= 100\n",
    "    df3['lte_uplink'] /= 100\n",
    "    df3.drop(['lte_category'], axis=1, inplace=True)\n",
    "    \n",
    "    df3.drop(['sim_size'], axis=1, inplace=True)\n",
    "    \n",
    "    df3['total_ram'] = df3['total_ram'].apply(split_external_storage_capacity)\n",
    "    df3['total_ram'].fillna(df3['total_ram'].median(), inplace=True)\n",
    "    df3['total_ram'] = np.log2(df3['total_ram'])\n",
    "    scaler = MinMaxScaler()\n",
    "    df3['total_ram'] = scaler.fit_transform(df3['total_ram'].values.reshape(-1,1))\n",
    "    df3['touch_screen'].fillna(0, inplace=True)\n",
    "    df3['wi_fi'].fillna(0, inplace=True)\n",
    "        \n",
    "    # years since release\n",
    "    df3 = pd.merge(df3, upgrades[['line_id', 'date_observed']].copy(), on='line_id')\n",
    "    df3['date_observed'] = pd.to_datetime(df3['date_observed'], format=\"%Y-%m-%d\").dt.year\n",
    "    \n",
    "    df3['years_since_release'] = df3['date_observed'] - df3['year_released']\n",
    "    df3['years_since_release'].fillna(df3['years_since_release'].dropna().median(), inplace=True)\n",
    "    df3['years_since_release'] = minmax_scale(df3['years_since_release'])\n",
    "    \n",
    "    df3.drop(['date_observed', 'year_released'], axis=1, inplace=True)\n",
    "    \n",
    "    cache(df3, filepath)\n",
    "    \n",
    "    return df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Redemptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract4(redemptions, upgrades, filepath=None, overwrite=False):\n",
    "    if is_cached(filepath) and not overwrite:\n",
    "        return load_cache(filepath)\n",
    "    \n",
    "    redemptions = redemptions.copy()\n",
    "    df4 = upgrades[['line_id']].copy()\n",
    "    \n",
    "    #num redemptions\n",
    "    num_redemptions = redemptions.groupby(['line_id']).size().to_frame('num_redemptions')\n",
    "    df4 = pd.merge(df4, num_redemptions, on='line_id', how='left')\n",
    "    df4['num_redemptions'].fillna(0, inplace=True)\n",
    "    df4['num_redemptions'] = np.log(df4['num_redemptions']+1)\n",
    "    scaler = MinMaxScaler()\n",
    "    df4['num_redemptions'] = scaler.fit_transform(df4['num_redemptions'].values.reshape(-1,1))\n",
    "    \n",
    "    # num one offs\n",
    "    one_off_threshold = 15\n",
    "    num_one_offs = redemptions[redemptions['gross_revenue'] <= one_off_threshold].groupby(['line_id']).size().to_frame('num_one_offs')\n",
    "    df4 = pd.merge(df4, num_one_offs, on='line_id', how='left')\n",
    "    df4['num_one_offs'].fillna(0, inplace=True)\n",
    "    df4['num_one_offs'] = np.log(df4['num_one_offs']+1)\n",
    "    scaler = MinMaxScaler()\n",
    "    df4['num_one_offs'] = scaler.fit_transform(df4['num_one_offs'].values.reshape(-1,1))\n",
    "    \n",
    "    # total spent\n",
    "    total_spent = redemptions.groupby(['line_id'])['gross_revenue'].sum().to_frame('total_spent')\n",
    "    df4 = pd.merge(df4, total_spent, on='line_id', how='left')\n",
    "    df4['total_spent'].fillna(0, inplace=True)\n",
    "    \n",
    "    # average monthly\n",
    "    redemptions['redemption_date'] = pd.to_datetime(redemptions['redemption_date'], format=\"%Y-%m-%d\")\n",
    "    date_start = redemptions.groupby(['line_id'])['redemption_date'].min().to_frame('date_start')\n",
    "    df4 = pd.merge(df4, date_start, on='line_id', how='left')\n",
    "    date_end = redemptions.groupby(['line_id'])['redemption_date'].max().to_frame('date_end')\n",
    "    df4 = pd.merge(df4, date_end, on='line_id', how='left')\n",
    "    df4['date_start'].fillna(np.datetime64('1970'), inplace=True)\n",
    "    df4['date_end'].fillna(np.datetime64('1970'), inplace=True)\n",
    "    \n",
    "    df4['total_months'] = (df4['date_end'] - df4['date_start']).dt.days//30+1\n",
    "    df4['average_monthly_spend'] = df4['total_spent'] / df4['total_months']\n",
    "    df4.drop(['date_start', 'date_end', 'total_months'], axis=1, inplace=True)\n",
    "    \n",
    "    df4['total_spent'] = np.log(df4['total_spent']+1)\n",
    "    scaler = StandardScaler()\n",
    "    df4['total_spent'] = scaler.fit_transform(df4['total_spent'].values.reshape(-1,1))\n",
    "    \n",
    "    df4['average_monthly_spend'] = np.log(df4['average_monthly_spend']+1)\n",
    "    scaler = MinMaxScaler()\n",
    "    df4['average_monthly_spend'] = scaler.fit_transform(df4['average_monthly_spend'].values.reshape(-1,1))\n",
    "    \n",
    "    # monthly plan cost\n",
    "    monthly_plan_cost = redemptions[redemptions['gross_revenue'] > one_off_threshold].groupby(['line_id']).median()\n",
    "    monthly_plan_cost = monthly_plan_cost.rename(columns={'gross_revenue':'monthly_plan_cost'})\n",
    "        \n",
    "    df4 = pd.merge(df4, monthly_plan_cost, on='line_id', how='left')\n",
    "    df4['monthly_plan_cost'].fillna(0, inplace=True)\n",
    "    df4['monthly_plan_cost'] = np.log(df4['num_one_offs']+1)\n",
    "    scaler = MinMaxScaler()\n",
    "    df4['monthly_plan_cost'] = scaler.fit_transform(df4['num_one_offs'].values.reshape(-1,1))\n",
    "    \n",
    "    cache(df4, filepath)\n",
    "    \n",
    "    return df4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Deactivations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract5(deactivations, upgrades, filepath=None, overwrite=False):\n",
    "    if is_cached(filepath) and not overwrite:\n",
    "        return load_cache(filepath)\n",
    "    \n",
    "    deactivations = deactivations.copy()\n",
    "    df5 = upgrades[['line_id']].copy()\n",
    "    \n",
    "    # has deactivated\n",
    "    has_deactivated = deactivations.groupby('line_id')['deactivation_reason'].any()\n",
    "    has_deactivated = has_deactivated.rename(f'has_deactivated')\n",
    "    df5 = pd.merge(df5, has_deactivated, on='line_id', how='left')\n",
    "    df5[f'has_deactivated'].replace({np.nan:0, True:1}, inplace=True)\n",
    "    \n",
    "    # has certain deactivation reasons\n",
    "    useful_values = [\"PASTDUE\", \"UPGRADE\", \"DEVICE CHANGE INQUIRY\", \"STOLEN\", \n",
    "                    \"DEFECTIVE\", \"ACTIVE UPGRADE\", \"REFURBISHED\", \"NO NEED OF PHONE\", \"DEVICERETURN\"]\n",
    "    for useful_value in useful_values: \n",
    "        has_value = deactivations[deactivations['deactivation_reason'] == useful_value].groupby('line_id')['deactivation_reason'].any()\n",
    "        has_value = has_value.rename(f'reason_{useful_value}')\n",
    "        df5 = pd.merge(df5, has_value, on='line_id', how='left')\n",
    "        df5[f'reason_{useful_value}'].replace({np.nan:0, True:1}, inplace=True)\n",
    "\n",
    "    # num deactivations\n",
    "    num_deactivations = deactivations.groupby(['line_id']).size().to_frame('num_deactivations')\n",
    "    df5 = pd.merge(df5, num_deactivations, on='line_id', how='left')\n",
    "    df5['num_deactivations'].fillna(0, inplace=True)\n",
    "    df5['num_deactivations'] = np.log(df5['num_deactivations']+1)\n",
    "    df5['num_deactivations'] = minmax_scale(df5['num_deactivations'])\n",
    "        \n",
    "    cache(df5, filepath)\n",
    "        \n",
    "    return df5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Reactivations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract6(reactivations, deactivations, upgrades, filepath=None, overwrite=False):\n",
    "    if is_cached(filepath) and not overwrite:\n",
    "        return load_cache(filepath)\n",
    "    \n",
    "    reactivations = reactivations.copy()\n",
    "    df6 = upgrades[['line_id']].copy()\n",
    "    \n",
    "    # check if active\n",
    "    df6['is_active'] = 1\n",
    "    \n",
    "    deactivations['deactivation_date'] = pd.to_datetime(deactivations['deactivation_date'], format=\"%Y-%m-%d\")\n",
    "    reactivations['reactivation_date'] = pd.to_datetime(reactivations['reactivation_date'], format=\"%Y-%m-%d\")\n",
    "    \n",
    "    last_deactivate = deactivations.groupby(['line_id'])['deactivation_date'].max().to_frame('last_deactivate')\n",
    "    last_reactivate = reactivations.groupby(['line_id'])['reactivation_date'].max().to_frame('last_reactivate')\n",
    "    \n",
    "    df6 = pd.merge(df6, last_deactivate, on='line_id', how='left')\n",
    "    df6 = pd.merge(df6, last_reactivate, on='line_id', how='left')\n",
    "    \n",
    "    has_deactivated = deactivations.groupby('line_id')['deactivation_reason'].any()\n",
    "    has_deactivated = has_deactivated.rename(f'has_deactivated')\n",
    "    df6 = pd.merge(df6, has_deactivated, on='line_id', how='left')\n",
    "    df6['has_deactivated'].replace({np.nan:0, True:1}, inplace=True)\n",
    "    df6['reactivated'] = np.where(df6['last_reactivate'] - df6['last_deactivate'] >= np.timedelta64(0, 'D'), 1, 0)\n",
    "    \n",
    "    df6['is_active'] = 1 - df6[f'has_deactivated'] + df6['reactivated']\n",
    "    \n",
    "    df6.drop(['last_deactivate', 'has_deactivated', 'reactivated'], axis=1, inplace=True)\n",
    "    \n",
    "    #num reactivations\n",
    "    num_reactivations = reactivations.groupby(['line_id']).size().to_frame('num_reactivations')\n",
    "    df6 = pd.merge(df6, num_reactivations, on='line_id', how='left')\n",
    "    df6['num_reactivations'].fillna(0, inplace=True)\n",
    "    df6['num_reactivations'] = np.log(df6['num_reactivations']+1)\n",
    "    df6['num_reactivations'] = minmax_scale(df6['num_reactivations'])\n",
    "    \n",
    "    # days since last reactivation\n",
    "    df6 = pd.merge(df6, upgrades[['line_id','date_observed']], on='line_id', how='left')\n",
    "    \n",
    "    df6['date_observed'] = convert_to_datetime(df6['date_observed'])\n",
    "    df6['days_since_reactivation'] = (df6['date_observed'] - df6['last_reactivate']).dt.days\n",
    "    df6['days_since_reactivation'] = minmax_scale(np.log(df6['days_since_reactivation']+1))\n",
    "    df6['days_since_reactivation'].fillna(0, inplace=True)\n",
    "    \n",
    "    df6.drop(['date_observed', 'last_reactivate'], axis=1, inplace=True)\n",
    "        \n",
    "    cache(df6, filepath)\n",
    "    \n",
    "    return df6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Suspensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract7(suspensions, upgrades, filepath=None, overwrite=False):\n",
    "    if is_cached(filepath) and not overwrite:\n",
    "        return load_cache(filepath)\n",
    "    \n",
    "    suspensions = suspensions.copy()\n",
    "    df7 = upgrades[['line_id', 'date_observed']].copy()\n",
    "    \n",
    "    # num and has been suspended\n",
    "    num_suspensions = suspensions.groupby(['line_id']).size().to_frame('num_suspensions')\n",
    "    df7 = pd.merge(df7, num_suspensions, on='line_id', how='left');\n",
    "    df7['num_suspensions'].fillna(0, inplace=True)\n",
    "    df7['has_been_suspended'] = np.where(df7['num_suspensions'] > 0, 1, 0)\n",
    "    df7['num_suspensions'] = minmax_scale(np.log(df7['num_suspensions']+1))\n",
    "    \n",
    "    # months since last suspension\n",
    "\n",
    "    df7['date_observed'] = convert_to_datetime(df7['date_observed'])\n",
    "    suspensions['suspension_start_date'] = convert_to_datetime(suspensions['suspension_start_date'])\n",
    "    suspensions['suspension_end_date'] = convert_to_datetime(suspensions['suspension_end_date'])\n",
    "\n",
    "    last_suspension_date = suspensions.groupby('line_id')['suspension_end_date'].max()\n",
    "    last_suspension_date.rename('last_suspension_date')\n",
    "    df7 = pd.merge(df7, last_suspension_date, on='line_id', how='left')\n",
    "    \n",
    "    df7['months_since_suspended'] = (df7['date_observed'] - df7['suspension_end_date']).dt.days/30+1\n",
    "    df7['months_since_suspended'].fillna(0, inplace=True)\n",
    "    df7['months_since_suspended'] = minmax_scale(np.log(df7['months_since_suspended']+1))\n",
    "    \n",
    "    df7.drop(['date_observed', 'suspension_end_date'], axis=1, inplace=True)\n",
    "    \n",
    "    # average suspension length\n",
    "    suspensions['suspension_length'] = (suspensions['suspension_end_date'] - suspensions['suspension_start_date']).dt.days\n",
    "    average_suspension_length = suspensions.groupby('line_id')['suspension_length'].mean().to_frame('average_suspension_length')\n",
    "    df7 = pd.merge(df7, average_suspension_length, on='line_id', how='left')\n",
    "    df7['average_suspension_length'].fillna(0, inplace=True)\n",
    "    df7['average_suspension_length'] = minmax_scale(df7['average_suspension_length']+1)\n",
    "    \n",
    "    cache(df7, filepath)\n",
    "    return df7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Network Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract8(network_usage, upgrades, filepath=None, overwrite=False):\n",
    "    if is_cached(filepath) and not overwrite:\n",
    "        return load_cache(filepath)\n",
    "    \n",
    "    network_usage = network_usage.copy()\n",
    "    df8 = upgrades[['line_id']].copy()\n",
    "    \n",
    "    # sms/mms in and out\n",
    "    print(\"  sms and mms...\")\n",
    "    sms_in_total = (network_usage.groupby('line_id')['mms_in'].sum() +\n",
    "         network_usage.groupby('line_id')['sms_in'].sum()).to_frame('sms_in_total')\n",
    "    df8 = pd.merge(df8, sms_in_total, on='line_id', how='left')\n",
    "    df8['sms_in_total'].fillna(0, inplace=True)    \n",
    "    \n",
    "    sms_out_total = (network_usage.groupby('line_id')['mms_out'].sum() +\n",
    "         network_usage.groupby('line_id')['sms_out'].sum()).to_frame('sms_out_total')\n",
    "    df8 = pd.merge(df8, sms_out_total, on='line_id', how='left')\n",
    "    df8['sms_out_total'].fillna(0, inplace=True)  \n",
    "    \n",
    "#     df8['sms_diff'] = df8['sms_in_total'] - df8['sms_out_total']\n",
    "    df8['sms_total'] = df8['sms_in_total'] + df8['sms_out_total']\n",
    "    \n",
    "    df8['over_15_sms'] = np.choose(df8['sms_total'] >= 15, [0, 1])\n",
    "    \n",
    "    # kilobytes of data\n",
    "    print(\"  kilobytes of data...\")\n",
    "    hotspot_kb = network_usage.groupby('line_id')['hotspot_kb'].sum()\n",
    "    df8 = pd.merge(df8, hotspot_kb, on='line_id', how='left')\n",
    "    df8['hotspot_kb'].fillna(0, inplace=True)  \n",
    "    df8['hotspot_gb'] = df8['hotspot_kb'] / 1024.0 / 1024.0\n",
    "    \n",
    "    kb_5g = network_usage.groupby('line_id')['kb_5g'].sum()\n",
    "    df8 = pd.merge(df8, kb_5g, on='line_id', how='left')\n",
    "    df8['kb_5g'].fillna(0, inplace=True) \n",
    "    df8['gb_5g'] = df8['kb_5g'] / 1024.0 / 1024.0\n",
    "             \n",
    "    total_kb = network_usage.groupby('line_id')['total_kb'].sum()\n",
    "    df8 = pd.merge(df8, total_kb, on='line_id', how='left')\n",
    "    df8['total_kb'].fillna(0, inplace=True)  \n",
    "    df8['total_gb'] = df8['total_kb'] / 1024.0 / 1024.0\n",
    "    \n",
    "    df8.drop(['hotspot_kb', 'kb_5g', 'total_kb'], axis=1, inplace=True)\n",
    "    \n",
    "    # voice counts\n",
    "    print(\"  voice counts...\")\n",
    "    voice_count_in = network_usage.groupby('line_id')['voice_count_in'].sum()\n",
    "    df8 = pd.merge(df8, voice_count_in, on='line_id', how='left')\n",
    "    df8['voice_count_in'].fillna(0, inplace=True) \n",
    "    \n",
    "    voice_count_total = network_usage.groupby('line_id')['voice_count_total'].sum()\n",
    "    df8 = pd.merge(df8, voice_count_total, on='line_id', how='left')\n",
    "    df8['voice_count_total'].fillna(0, inplace=True) \n",
    "    \n",
    "    df8['voice_count_out'] = df8['voice_count_total'] - df8['voice_count_in']\n",
    "#     df8['voice_count_diff'] = df8['voice_count_in'] - df8['voice_count_out']  \n",
    "        \n",
    "    df8['over_5_voice'] = np.choose(df8['voice_count_total'] >= 5, [0, 1])\n",
    "        \n",
    "    # voice minutes\n",
    "    print(\"  voice minutes..\")\n",
    "    voice_min_in = network_usage.groupby('line_id')['voice_min_in'].sum()\n",
    "    df8 = pd.merge(df8, voice_min_in, on='line_id', how='left')\n",
    "    df8['voice_min_in'].fillna(0, inplace=True) \n",
    "    \n",
    "    voice_min_out = network_usage.groupby('line_id')['voice_min_out'].sum()\n",
    "    df8 = pd.merge(df8, voice_min_out, on='line_id', how='left')\n",
    "    df8['voice_min_out'].fillna(0, inplace=True) \n",
    "    \n",
    "    df8['voice_min_total'] = df8['voice_min_in'] + df8['voice_min_out']\n",
    "    \n",
    "    # average call length\n",
    "    df8['average_call_length'] = np.choose(\n",
    "        df8['voice_count_total'] == 0, \n",
    "        [df8['voice_min_total'] / df8['voice_count_total'], 0]\n",
    "    )\n",
    "    \n",
    "    #num usages\n",
    "    num_network_usages = network_usage.groupby(['line_id']).size().to_frame('num_network_usages')\n",
    "    df8 = pd.merge(df8, num_network_usages, on='line_id', how='left')\n",
    "    df8['num_network_usages'].fillna(0, inplace=True)\n",
    "    df8['num_network_usages'] = np.log(df8['num_network_usages']+1)\n",
    "    df8['num_network_usages'] = minmax_scale(df8['num_network_usages'])\n",
    "    \n",
    "    print(\"  scaling all parameters..\")\n",
    "    df8['sms_in_total'] = standard_scale(np.log(df8['sms_in_total']+1))\n",
    "    df8['sms_out_total'] = standard_scale(np.log(df8['sms_out_total']+1))\n",
    "    df8['sms_total'] = standard_scale(np.log(df8['sms_total']+1))\n",
    "#     df8['sms_diff'] = standard_scale(df8['sms_diff'])\n",
    "    df8['hotspot_gb'] = standard_scale(np.log(df8['hotspot_gb']+1))\n",
    "    df8['gb_5g'] = standard_scale(np.log(df8['gb_5g']+1))\n",
    "    df8['total_gb'] = standard_scale(np.log(df8['total_gb']+1))\n",
    "    df8['voice_count_in'] = standard_scale(np.log(df8['voice_count_in']+1))\n",
    "    df8['voice_count_out'] = standard_scale(np.log(df8['voice_count_out']+1))\n",
    "    df8['voice_count_total'] = standard_scale(np.log(df8['voice_count_total']+1))\n",
    "    df8['voice_min_in'] = standard_scale(np.log(df8['voice_min_in']+1))\n",
    "    df8['voice_min_out'] = standard_scale(np.log(df8['voice_min_out']+1))\n",
    "    df8['voice_min_total'] = standard_scale(np.log(df8['voice_min_total']+1))\n",
    "#     df8['voice_count_diff'] = standard_scale(df8['voice_count_diff'])\n",
    "    df8['average_call_length'] = standard_scale(np.log(df8['average_call_length']+1))\n",
    "    \n",
    "    cache(df8, filepath)\n",
    "    \n",
    "    return df8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. LRP Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract9(lrp_points, upgrades, filepath=None, overwrite=False):\n",
    "    if is_cached(filepath) and not overwrite:\n",
    "        return load_cache(filepath)\n",
    "    \n",
    "    lrp_points = lrp_points.copy()\n",
    "    df9 = upgrades[['line_id']].copy()\n",
    "    \n",
    "    df9 = pd.merge(df9, lrp_points, on='line_id', how='left')\n",
    "    \n",
    "    # total_quantity\n",
    "    df9['total_quantity'].fillna(0, inplace=True)\n",
    "\n",
    "    # has_earned_points\n",
    "    df9['has_earned_points'] = np.where(df9['status'] == 'ENROLLED', 1, 0)\n",
    "    \n",
    "    # total_spent\n",
    "    df9['total_spent_lrp'] = df9['total_quantity'] - df9['quantity']\n",
    "    df9['total_spent_lrp'].fillna(0, inplace=True)\n",
    "    \n",
    "    # has_spent\n",
    "    df9['has_spent'] = np.where(df9['total_spent_lrp'] > 0, 1, 0)\n",
    "    \n",
    "    # days_since_earnt\n",
    "    df9['update_date'] = convert_to_datetime(df9['update_date'])\n",
    "    df9 = pd.merge(df9, upgrades[['line_id', 'date_observed']].copy(), on='line_id', how='left')\n",
    "    df9['date_observed'] = convert_to_datetime(df9['date_observed'])\n",
    "    \n",
    "    df9['days_since_update'] = (df9['date_observed'] - df9['update_date']).dt.days\n",
    "    df9['days_since_update'].fillna(0, inplace=True)\n",
    "    df9['days_since_update'] = minmax_scale(np.log(df9['days_since_update']+1))\n",
    "    \n",
    "    df9.drop(['quantity', 'status', 'update_date', 'date_observed'], axis=1, inplace=True)\n",
    "\n",
    "    cache(df9, filepath)\n",
    "    return df9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. LRP Enrollment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract10(lrp_enrollment, upgrades, filepath=None, overwrite=False):\n",
    "    if is_cached(filepath) and not overwrite:\n",
    "        return load_cache(filepath)\n",
    "    \n",
    "    lrp_enrollment = lrp_enrollment.copy()\n",
    "    df10 = upgrades[['line_id', 'date_observed']].copy()\n",
    "    \n",
    "    num_enrols = lrp_enrollment.groupby('line_id').size().to_frame('num_enrols')\n",
    "    df10 = pd.merge(df10, num_enrols, on='line_id', how='left')\n",
    "    df10['num_enrols'].fillna(0, inplace=True)\n",
    "    df10['has_enrolled'] = np.where(df10['num_enrols'] > 0, 1, 0)\n",
    "    df10['has_enrolled_over_twice'] = np.where(df10['num_enrols'] >= 2, 1, 0)\n",
    "    \n",
    "    df10['num_enrols'] = minmax_scale(df10['num_enrols'])\n",
    "    \n",
    "    # num months since enrolment\n",
    "    df10['date_observed'] = convert_to_datetime(df10['date_observed'])\n",
    "    lrp_enrollment['lrp_enrollment_date'] = convert_to_datetime(lrp_enrollment['lrp_enrollment_date'])\n",
    "    \n",
    "    last_enrolled_date = lrp_enrollment.groupby('line_id')['lrp_enrollment_date'].max()\n",
    "    df10 = pd.merge(df10, last_enrolled_date, on='line_id', how='left')\n",
    "    \n",
    "    df10['months_since_enrolled'] = (df10['date_observed'] - df10['lrp_enrollment_date']).dt.days/30+1\n",
    "    df10['months_since_enrolled'].fillna(0, inplace=True)\n",
    "    df10['months_since_enrolled'] = standard_scale(np.log(df10['months_since_enrolled']+1))\n",
    "    \n",
    "    df10.drop(['date_observed', 'lrp_enrollment_date'], axis=1, inplace=True)\n",
    "    \n",
    "    cache(df10, filepath)\n",
    "    return df10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data=\"dev\"):\n",
    "    assert(data == \"dev\" or data == \"eval\")\n",
    "    \n",
    "    if data == \"dev\":\n",
    "        base_folder = data_folder+\"data/dev/\"\n",
    "    elif data == \"eval\":\n",
    "        base_folder = data_folder+\"data/eval/\"\n",
    "        \n",
    "    print(\"loading upgrades...\")\n",
    "    upgrades = pd.read_csv(base_folder + \"upgrades.csv\")\n",
    "    print(\"loading customer_info...\")\n",
    "    customer_info = pd.read_csv(base_folder + \"customer_info.csv\")\n",
    "    print(\"loading phone_info...\")\n",
    "    phone_info = pd.read_csv(base_folder + \"phone_info.csv\")\n",
    "    print(\"loading redemptions...\")\n",
    "    redemptions = pd.read_csv(base_folder + \"redemptions.csv\")\n",
    "    print(\"loading deactivations...\")\n",
    "    deactivations = pd.read_csv(base_folder + \"deactivations.csv\")\n",
    "    print(\"loading reactivations...\")\n",
    "    reactivations = pd.read_csv(base_folder + \"reactivations.csv\")\n",
    "    print(\"loading suspensions...\")\n",
    "    suspensions = pd.read_csv(base_folder + \"suspensions.csv\")\n",
    "    print(\"loading network_usage_domestic...\")\n",
    "    network_usage_domestic = pd.read_csv(base_folder + \"network_usage_domestic.csv\")\n",
    "    print(\"loading lrp_points...\")\n",
    "    lrp_points = pd.read_csv(base_folder + \"lrp_points.csv\")\n",
    "    print(\"loading lrp_enrollment...\")\n",
    "    lrp_enrollment = pd.read_csv(base_folder + \"lrp_enrollment.csv\")\n",
    "    \n",
    "    print(\"done!\")\n",
    "    \n",
    "    return upgrades, customer_info, phone_info, redemptions, deactivations, reactivations, \\\n",
    "        suspensions, network_usage_domestic, lrp_points, lrp_enrollment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading upgrades...\n",
      "loading customer_info...\n",
      "loading phone_info...\n",
      "loading redemptions...\n",
      "loading deactivations...\n",
      "loading reactivations...\n",
      "loading suspensions...\n",
      "loading network_usage_domestic...\n",
      "loading lrp_points...\n",
      "loading lrp_enrollment...\n",
      "done!\n",
      "loading upgrades...\n",
      "loading customer_info...\n",
      "loading phone_info...\n",
      "loading redemptions...\n",
      "loading deactivations...\n",
      "loading reactivations...\n",
      "loading suspensions...\n",
      "loading network_usage_domestic...\n",
      "loading lrp_points...\n",
      "loading lrp_enrollment...\n",
      "done!\n",
      "length of dev: 10\n",
      "length of eval: 10\n"
     ]
    }
   ],
   "source": [
    "data_dev = load_data('dev')\n",
    "data_eval = load_data('eval')\n",
    "\n",
    "print(\"length of dev:\", len(data_dev))\n",
    "print(\"length of eval:\", len(data_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(datasets, data=\"dev\", overwrite=False):\n",
    "    assert(data == \"dev\" or data == \"eval\")\n",
    "    \n",
    "    if data == \"dev\":\n",
    "        base_path = root_folder+\"code/pickles/{}_dev.pickle\"\n",
    "        output_path = root_folder+\"code/dev-extracted.csv\"\n",
    "        \n",
    "    elif data == \"eval\":\n",
    "        base_path = root_folder+\"code/pickles/{}_eval.pickle\"\n",
    "        output_path = root_folder+\"code/eval-extracted.csv\"\n",
    "        \n",
    "    (upgrades, customer_info, phone_info, redemptions, deactivations, reactivations, \\\n",
    "        suspensions, network_usage_domestic, lrp_points, lrp_enrollment) = datasets\n",
    "    \n",
    "    print(\"extracting 1...\")\n",
    "    df1 = extract1(upgrades)\n",
    "    print(\"extracting 2...\")\n",
    "    df2 = extract2(customer_info, upgrades)\n",
    "    print(\"extracting 3...\")\n",
    "    df3 = extract3(phone_info, upgrades, filepath=base_path.format(3), overwrite=overwrite)\n",
    "    print(\"extracting 4...\")\n",
    "    df4 = extract4(redemptions, upgrades, filepath=base_path.format(4), overwrite=overwrite)\n",
    "    print(\"extracting 5...\")\n",
    "    df5 = extract5(deactivations, upgrades, filepath=base_path.format(5), overwrite=overwrite)\n",
    "    print(\"extracting 6...\")\n",
    "    df6 = extract6(reactivations, deactivations, upgrades, filepath=base_path.format(6), overwrite=overwrite)\n",
    "    print(\"extracting 7...\")\n",
    "    df7 = extract7(suspensions, upgrades, filepath=base_path.format(7), overwrite=overwrite)\n",
    "    print(\"extracting 8...\")\n",
    "    df8 = extract8(network_usage_domestic, upgrades, filepath=base_path.format(8), overwrite=overwrite)\n",
    "    print(\"extracting 9...\")\n",
    "    df9 = extract9(lrp_points, upgrades, filepath=base_path.format(9), overwrite=overwrite)\n",
    "    print(\"extracting 10...\")\n",
    "    df10 = extract10(lrp_enrollment, upgrades, filepath=base_path.format(10), overwrite=overwrite)\n",
    "    \n",
    "    print(\"extracted all features\")\n",
    "    \n",
    "    print(\"merging them all together...\")\n",
    "    dfs = [df1, df2, df3, df4, df5, df6, df7, df8, df9, df10]\n",
    "    df = dfs[0].copy()\n",
    "    for i in range(1, len(dfs)):\n",
    "        assert(len(dfs[i]) == len(df))\n",
    "        df = pd.merge(df, dfs[i], on='line_id')\n",
    "    \n",
    "    assert(df.isna().any().any() == False)\n",
    "        \n",
    "    print(\"saving to file...\")\n",
    "    print(output_path)\n",
    "    df.to_csv(output_path, header=True, index=None)\n",
    "    \n",
    "    print(\"done!\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting 1...\n",
      "extracting 2...\n",
      "extracting 3...\n",
      "extracting 4...\n",
      "extracting 5...\n",
      "extracting 6...\n",
      "extracting 7...\n",
      "extracting 8...\n",
      "  sms and mms...\n",
      "  kilobytes of data...\n",
      "  voice counts...\n",
      "  voice minutes..\n",
      "  scaling all parameters..\n",
      "extracting 9...\n",
      "extracting 10...\n",
      "extracted all features\n",
      "merging them all together...\n",
      "saving to file...\n",
      "s3://tf-trachack-notebooks/emotional-support-vector-machine-unsw/jupyter/jovyan/code/dev-extracted.csv\n",
      "done!\n",
      "extracting 1...\n",
      "extracting 2...\n",
      "extracting 3...\n",
      "extracting 4...\n",
      "extracting 5...\n",
      "extracting 6...\n",
      "extracting 7...\n",
      "extracting 8...\n",
      "  sms and mms...\n",
      "  kilobytes of data...\n",
      "  voice counts...\n",
      "  voice minutes..\n",
      "  scaling all parameters..\n",
      "extracting 9...\n",
      "extracting 10...\n",
      "extracted all features\n",
      "merging them all together...\n",
      "saving to file...\n",
      "s3://tf-trachack-notebooks/emotional-support-vector-machine-unsw/jupyter/jovyan/code/eval-extracted.csv\n",
      "done!\n",
      "number of features: 77\n"
     ]
    }
   ],
   "source": [
    "data_dev_extracted = extract_features(data_dev, 'dev', overwrite=True)\n",
    "data_eval_extracted = extract_features(data_eval, 'eval', overwrite=True)\n",
    "\n",
    "print(\"number of features:\", len(data_dev_extracted.dtypes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num features: 75\n",
      "num train samples: 44694\n",
      "num test samples: 11174\n"
     ]
    }
   ],
   "source": [
    "y = data_dev_extracted['upgrade']\n",
    "X = data_dev_extracted.drop(['line_id', 'upgrade'], axis = 1)\n",
    "\n",
    "print(\"num features:\", len(X.dtypes))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "print(\"num train samples:\", len(X_train))\n",
    "print(\"num test samples:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training LGBM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15174, number of negative: 40694\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011050 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5931\n",
      "[LightGBM] [Info] Number of data points in the train set: 55868, number of used features: 69\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.271604 -> initscore=-0.986497\n",
      "[LightGBM] [Info] Start training from score -0.986497\n",
      "[1]\tvalid_0's binary_logloss: 0.514494\n",
      "[2]\tvalid_0's binary_logloss: 0.463968\n",
      "[3]\tvalid_0's binary_logloss: 0.423555\n",
      "[4]\tvalid_0's binary_logloss: 0.389695\n",
      "[5]\tvalid_0's binary_logloss: 0.361414\n",
      "[6]\tvalid_0's binary_logloss: 0.337302\n",
      "[7]\tvalid_0's binary_logloss: 0.316563\n",
      "[8]\tvalid_0's binary_logloss: 0.297964\n",
      "[9]\tvalid_0's binary_logloss: 0.282221\n",
      "[10]\tvalid_0's binary_logloss: 0.268217\n",
      "[11]\tvalid_0's binary_logloss: 0.255808\n",
      "[12]\tvalid_0's binary_logloss: 0.244502\n",
      "[13]\tvalid_0's binary_logloss: 0.234341\n",
      "[14]\tvalid_0's binary_logloss: 0.22539\n",
      "[15]\tvalid_0's binary_logloss: 0.216902\n",
      "[16]\tvalid_0's binary_logloss: 0.209317\n",
      "[17]\tvalid_0's binary_logloss: 0.202141\n",
      "[18]\tvalid_0's binary_logloss: 0.196078\n",
      "[19]\tvalid_0's binary_logloss: 0.190109\n",
      "[20]\tvalid_0's binary_logloss: 0.18492\n",
      "[21]\tvalid_0's binary_logloss: 0.180241\n",
      "[22]\tvalid_0's binary_logloss: 0.175698\n",
      "[23]\tvalid_0's binary_logloss: 0.171447\n",
      "[24]\tvalid_0's binary_logloss: 0.167343\n",
      "[25]\tvalid_0's binary_logloss: 0.163471\n",
      "[26]\tvalid_0's binary_logloss: 0.159935\n",
      "[27]\tvalid_0's binary_logloss: 0.15629\n",
      "[28]\tvalid_0's binary_logloss: 0.153129\n",
      "[29]\tvalid_0's binary_logloss: 0.150045\n",
      "[30]\tvalid_0's binary_logloss: 0.146852\n",
      "[31]\tvalid_0's binary_logloss: 0.144\n",
      "[32]\tvalid_0's binary_logloss: 0.141431\n",
      "[33]\tvalid_0's binary_logloss: 0.138779\n",
      "[34]\tvalid_0's binary_logloss: 0.136281\n",
      "[35]\tvalid_0's binary_logloss: 0.134013\n",
      "[36]\tvalid_0's binary_logloss: 0.131918\n",
      "[37]\tvalid_0's binary_logloss: 0.129715\n",
      "[38]\tvalid_0's binary_logloss: 0.127423\n",
      "[39]\tvalid_0's binary_logloss: 0.125057\n",
      "[40]\tvalid_0's binary_logloss: 0.123173\n",
      "[41]\tvalid_0's binary_logloss: 0.121182\n",
      "[42]\tvalid_0's binary_logloss: 0.119173\n",
      "[43]\tvalid_0's binary_logloss: 0.117403\n",
      "[44]\tvalid_0's binary_logloss: 0.115635\n",
      "[45]\tvalid_0's binary_logloss: 0.113867\n",
      "[46]\tvalid_0's binary_logloss: 0.111985\n",
      "[47]\tvalid_0's binary_logloss: 0.110366\n",
      "[48]\tvalid_0's binary_logloss: 0.108949\n",
      "[49]\tvalid_0's binary_logloss: 0.107429\n",
      "[50]\tvalid_0's binary_logloss: 0.106033\n",
      "[51]\tvalid_0's binary_logloss: 0.104753\n",
      "[52]\tvalid_0's binary_logloss: 0.103371\n",
      "[53]\tvalid_0's binary_logloss: 0.101964\n",
      "[54]\tvalid_0's binary_logloss: 0.100668\n",
      "[55]\tvalid_0's binary_logloss: 0.0994915\n",
      "[56]\tvalid_0's binary_logloss: 0.0981185\n",
      "[57]\tvalid_0's binary_logloss: 0.0968986\n",
      "[58]\tvalid_0's binary_logloss: 0.0955994\n",
      "[59]\tvalid_0's binary_logloss: 0.0943908\n",
      "[60]\tvalid_0's binary_logloss: 0.0931615\n",
      "[61]\tvalid_0's binary_logloss: 0.0920272\n",
      "[62]\tvalid_0's binary_logloss: 0.0907761\n",
      "[63]\tvalid_0's binary_logloss: 0.0896783\n",
      "[64]\tvalid_0's binary_logloss: 0.0887941\n",
      "[65]\tvalid_0's binary_logloss: 0.087841\n",
      "[66]\tvalid_0's binary_logloss: 0.08679\n",
      "[67]\tvalid_0's binary_logloss: 0.0856789\n",
      "[68]\tvalid_0's binary_logloss: 0.0847518\n",
      "[69]\tvalid_0's binary_logloss: 0.0838129\n",
      "[70]\tvalid_0's binary_logloss: 0.0830683\n",
      "[71]\tvalid_0's binary_logloss: 0.0821237\n",
      "[72]\tvalid_0's binary_logloss: 0.0811259\n",
      "[73]\tvalid_0's binary_logloss: 0.0801659\n",
      "[74]\tvalid_0's binary_logloss: 0.0792451\n",
      "[75]\tvalid_0's binary_logloss: 0.0783498\n",
      "[76]\tvalid_0's binary_logloss: 0.0775922\n",
      "[77]\tvalid_0's binary_logloss: 0.0767434\n",
      "[78]\tvalid_0's binary_logloss: 0.0759937\n",
      "[79]\tvalid_0's binary_logloss: 0.075217\n",
      "[80]\tvalid_0's binary_logloss: 0.0745922\n",
      "[81]\tvalid_0's binary_logloss: 0.0737867\n",
      "[82]\tvalid_0's binary_logloss: 0.073091\n",
      "[83]\tvalid_0's binary_logloss: 0.0721924\n",
      "[84]\tvalid_0's binary_logloss: 0.0713364\n",
      "[85]\tvalid_0's binary_logloss: 0.0706331\n",
      "[86]\tvalid_0's binary_logloss: 0.0700715\n",
      "[87]\tvalid_0's binary_logloss: 0.0692802\n",
      "[88]\tvalid_0's binary_logloss: 0.0685305\n",
      "[89]\tvalid_0's binary_logloss: 0.0678078\n",
      "[90]\tvalid_0's binary_logloss: 0.0671432\n",
      "[91]\tvalid_0's binary_logloss: 0.0664991\n",
      "[92]\tvalid_0's binary_logloss: 0.0658431\n",
      "[93]\tvalid_0's binary_logloss: 0.0652493\n",
      "[94]\tvalid_0's binary_logloss: 0.064562\n",
      "[95]\tvalid_0's binary_logloss: 0.0640937\n",
      "[96]\tvalid_0's binary_logloss: 0.0635412\n",
      "[97]\tvalid_0's binary_logloss: 0.0628263\n",
      "[98]\tvalid_0's binary_logloss: 0.0620998\n",
      "[99]\tvalid_0's binary_logloss: 0.0615127\n",
      "[100]\tvalid_0's binary_logloss: 0.0609386\n",
      "[101]\tvalid_0's binary_logloss: 0.0602666\n",
      "[102]\tvalid_0's binary_logloss: 0.0598256\n",
      "[103]\tvalid_0's binary_logloss: 0.0592018\n",
      "[104]\tvalid_0's binary_logloss: 0.0586916\n",
      "[105]\tvalid_0's binary_logloss: 0.0580918\n",
      "[106]\tvalid_0's binary_logloss: 0.0574606\n",
      "[107]\tvalid_0's binary_logloss: 0.0569088\n",
      "[108]\tvalid_0's binary_logloss: 0.0564848\n",
      "[109]\tvalid_0's binary_logloss: 0.0560316\n",
      "[110]\tvalid_0's binary_logloss: 0.0554756\n",
      "[111]\tvalid_0's binary_logloss: 0.0549385\n",
      "[112]\tvalid_0's binary_logloss: 0.0542547\n",
      "[113]\tvalid_0's binary_logloss: 0.0537472\n",
      "[114]\tvalid_0's binary_logloss: 0.0532982\n",
      "[115]\tvalid_0's binary_logloss: 0.0527826\n",
      "[116]\tvalid_0's binary_logloss: 0.0521802\n",
      "[117]\tvalid_0's binary_logloss: 0.0516671\n",
      "[118]\tvalid_0's binary_logloss: 0.0512603\n",
      "[119]\tvalid_0's binary_logloss: 0.0508492\n",
      "[120]\tvalid_0's binary_logloss: 0.050402\n",
      "[121]\tvalid_0's binary_logloss: 0.0500189\n",
      "[122]\tvalid_0's binary_logloss: 0.0495254\n",
      "[123]\tvalid_0's binary_logloss: 0.0491205\n",
      "[124]\tvalid_0's binary_logloss: 0.0488084\n",
      "[125]\tvalid_0's binary_logloss: 0.0484319\n",
      "[126]\tvalid_0's binary_logloss: 0.0480042\n",
      "[127]\tvalid_0's binary_logloss: 0.0474777\n",
      "[128]\tvalid_0's binary_logloss: 0.0469944\n",
      "[129]\tvalid_0's binary_logloss: 0.0465898\n",
      "[130]\tvalid_0's binary_logloss: 0.0461998\n",
      "[131]\tvalid_0's binary_logloss: 0.0457218\n",
      "[132]\tvalid_0's binary_logloss: 0.0452782\n",
      "[133]\tvalid_0's binary_logloss: 0.0448483\n",
      "[134]\tvalid_0's binary_logloss: 0.0445128\n",
      "[135]\tvalid_0's binary_logloss: 0.0440112\n",
      "[136]\tvalid_0's binary_logloss: 0.0436293\n",
      "[137]\tvalid_0's binary_logloss: 0.0433578\n",
      "[138]\tvalid_0's binary_logloss: 0.0430249\n",
      "[139]\tvalid_0's binary_logloss: 0.0426049\n",
      "[140]\tvalid_0's binary_logloss: 0.0421724\n",
      "[141]\tvalid_0's binary_logloss: 0.0419339\n",
      "[142]\tvalid_0's binary_logloss: 0.0415361\n",
      "[143]\tvalid_0's binary_logloss: 0.0412305\n",
      "[144]\tvalid_0's binary_logloss: 0.0409478\n",
      "[145]\tvalid_0's binary_logloss: 0.0405384\n",
      "[146]\tvalid_0's binary_logloss: 0.0402211\n",
      "[147]\tvalid_0's binary_logloss: 0.0399231\n",
      "[148]\tvalid_0's binary_logloss: 0.0396165\n",
      "[149]\tvalid_0's binary_logloss: 0.0392961\n",
      "[150]\tvalid_0's binary_logloss: 0.0390172\n",
      "[151]\tvalid_0's binary_logloss: 0.0386855\n",
      "[152]\tvalid_0's binary_logloss: 0.0384363\n",
      "[153]\tvalid_0's binary_logloss: 0.038074\n",
      "[154]\tvalid_0's binary_logloss: 0.0377926\n",
      "[155]\tvalid_0's binary_logloss: 0.0374017\n",
      "[156]\tvalid_0's binary_logloss: 0.0370642\n",
      "[157]\tvalid_0's binary_logloss: 0.0367339\n",
      "[158]\tvalid_0's binary_logloss: 0.0364495\n",
      "[159]\tvalid_0's binary_logloss: 0.0361437\n",
      "[160]\tvalid_0's binary_logloss: 0.0357628\n",
      "[161]\tvalid_0's binary_logloss: 0.0355248\n",
      "[162]\tvalid_0's binary_logloss: 0.0353044\n",
      "[163]\tvalid_0's binary_logloss: 0.0351207\n",
      "[164]\tvalid_0's binary_logloss: 0.0348356\n",
      "[165]\tvalid_0's binary_logloss: 0.0346064\n",
      "[166]\tvalid_0's binary_logloss: 0.0343983\n",
      "[167]\tvalid_0's binary_logloss: 0.0341344\n",
      "[168]\tvalid_0's binary_logloss: 0.0337915\n",
      "[169]\tvalid_0's binary_logloss: 0.0336074\n",
      "[170]\tvalid_0's binary_logloss: 0.0333816\n",
      "[171]\tvalid_0's binary_logloss: 0.0330727\n",
      "[172]\tvalid_0's binary_logloss: 0.0327742\n",
      "[173]\tvalid_0's binary_logloss: 0.0325525\n",
      "[174]\tvalid_0's binary_logloss: 0.032258\n",
      "[175]\tvalid_0's binary_logloss: 0.0319756\n",
      "[176]\tvalid_0's binary_logloss: 0.0316674\n",
      "[177]\tvalid_0's binary_logloss: 0.0314973\n",
      "[178]\tvalid_0's binary_logloss: 0.0313064\n",
      "[179]\tvalid_0's binary_logloss: 0.0310028\n",
      "[180]\tvalid_0's binary_logloss: 0.0308149\n",
      "[181]\tvalid_0's binary_logloss: 0.0305399\n",
      "[182]\tvalid_0's binary_logloss: 0.0302476\n",
      "[183]\tvalid_0's binary_logloss: 0.0299914\n",
      "[184]\tvalid_0's binary_logloss: 0.0297274\n",
      "[185]\tvalid_0's binary_logloss: 0.0295744\n",
      "[186]\tvalid_0's binary_logloss: 0.0293241\n",
      "[187]\tvalid_0's binary_logloss: 0.0291431\n",
      "[188]\tvalid_0's binary_logloss: 0.0289278\n",
      "[189]\tvalid_0's binary_logloss: 0.0287604\n",
      "[190]\tvalid_0's binary_logloss: 0.0285972\n",
      "[191]\tvalid_0's binary_logloss: 0.0283587\n",
      "[192]\tvalid_0's binary_logloss: 0.0281147\n",
      "[193]\tvalid_0's binary_logloss: 0.0279557\n",
      "[194]\tvalid_0's binary_logloss: 0.02772\n",
      "[195]\tvalid_0's binary_logloss: 0.0275887\n",
      "[196]\tvalid_0's binary_logloss: 0.0273401\n",
      "[197]\tvalid_0's binary_logloss: 0.0270908\n",
      "[198]\tvalid_0's binary_logloss: 0.026892\n",
      "[199]\tvalid_0's binary_logloss: 0.0267519\n",
      "[200]\tvalid_0's binary_logloss: 0.0265496\n",
      "[201]\tvalid_0's binary_logloss: 0.0263847\n",
      "[202]\tvalid_0's binary_logloss: 0.0261811\n",
      "[203]\tvalid_0's binary_logloss: 0.025991\n",
      "[204]\tvalid_0's binary_logloss: 0.0257927\n",
      "[205]\tvalid_0's binary_logloss: 0.0255765\n",
      "[206]\tvalid_0's binary_logloss: 0.0253934\n",
      "[207]\tvalid_0's binary_logloss: 0.0251693\n",
      "[208]\tvalid_0's binary_logloss: 0.0249191\n",
      "[209]\tvalid_0's binary_logloss: 0.0247053\n",
      "[210]\tvalid_0's binary_logloss: 0.0245372\n",
      "[211]\tvalid_0's binary_logloss: 0.0244402\n",
      "[212]\tvalid_0's binary_logloss: 0.0242152\n",
      "[213]\tvalid_0's binary_logloss: 0.0240415\n",
      "[214]\tvalid_0's binary_logloss: 0.0238308\n",
      "[215]\tvalid_0's binary_logloss: 0.0236192\n",
      "[216]\tvalid_0's binary_logloss: 0.0234629\n",
      "[217]\tvalid_0's binary_logloss: 0.0233027\n",
      "[218]\tvalid_0's binary_logloss: 0.0231679\n",
      "[219]\tvalid_0's binary_logloss: 0.0230194\n",
      "[220]\tvalid_0's binary_logloss: 0.0228957\n",
      "[221]\tvalid_0's binary_logloss: 0.0227453\n",
      "[222]\tvalid_0's binary_logloss: 0.0226369\n",
      "[223]\tvalid_0's binary_logloss: 0.0224956\n",
      "[224]\tvalid_0's binary_logloss: 0.0223728\n",
      "[225]\tvalid_0's binary_logloss: 0.0222044\n",
      "[226]\tvalid_0's binary_logloss: 0.0220245\n",
      "[227]\tvalid_0's binary_logloss: 0.0219234\n",
      "[228]\tvalid_0's binary_logloss: 0.0218066\n",
      "[229]\tvalid_0's binary_logloss: 0.0216111\n",
      "[230]\tvalid_0's binary_logloss: 0.0214736\n",
      "[231]\tvalid_0's binary_logloss: 0.0213374\n",
      "[232]\tvalid_0's binary_logloss: 0.0211557\n",
      "[233]\tvalid_0's binary_logloss: 0.0210152\n",
      "[234]\tvalid_0's binary_logloss: 0.0208684\n",
      "[235]\tvalid_0's binary_logloss: 0.0207433\n",
      "[236]\tvalid_0's binary_logloss: 0.0206428\n",
      "[237]\tvalid_0's binary_logloss: 0.0205029\n",
      "[238]\tvalid_0's binary_logloss: 0.0204162\n",
      "[239]\tvalid_0's binary_logloss: 0.0202923\n",
      "[240]\tvalid_0's binary_logloss: 0.0201725\n",
      "[241]\tvalid_0's binary_logloss: 0.0200277\n",
      "[242]\tvalid_0's binary_logloss: 0.0198524\n",
      "[243]\tvalid_0's binary_logloss: 0.0197085\n",
      "[244]\tvalid_0's binary_logloss: 0.019559\n",
      "[245]\tvalid_0's binary_logloss: 0.0194396\n",
      "[246]\tvalid_0's binary_logloss: 0.019278\n",
      "[247]\tvalid_0's binary_logloss: 0.0191424\n",
      "[248]\tvalid_0's binary_logloss: 0.0190141\n",
      "[249]\tvalid_0's binary_logloss: 0.0188574\n",
      "[250]\tvalid_0's binary_logloss: 0.0187646\n",
      "[251]\tvalid_0's binary_logloss: 0.0186507\n",
      "[252]\tvalid_0's binary_logloss: 0.0185627\n",
      "[253]\tvalid_0's binary_logloss: 0.018448\n",
      "[254]\tvalid_0's binary_logloss: 0.0183483\n",
      "[255]\tvalid_0's binary_logloss: 0.0182323\n",
      "[256]\tvalid_0's binary_logloss: 0.0180949\n",
      "[257]\tvalid_0's binary_logloss: 0.0180143\n",
      "[258]\tvalid_0's binary_logloss: 0.0179445\n",
      "[259]\tvalid_0's binary_logloss: 0.0178157\n",
      "[260]\tvalid_0's binary_logloss: 0.017694\n",
      "[261]\tvalid_0's binary_logloss: 0.0176109\n",
      "[262]\tvalid_0's binary_logloss: 0.0175065\n",
      "[263]\tvalid_0's binary_logloss: 0.0174202\n",
      "[264]\tvalid_0's binary_logloss: 0.0173215\n",
      "[265]\tvalid_0's binary_logloss: 0.0172446\n",
      "[266]\tvalid_0's binary_logloss: 0.0171623\n",
      "[267]\tvalid_0's binary_logloss: 0.0170696\n",
      "[268]\tvalid_0's binary_logloss: 0.0169921\n",
      "[269]\tvalid_0's binary_logloss: 0.0168775\n",
      "[270]\tvalid_0's binary_logloss: 0.0168111\n",
      "[271]\tvalid_0's binary_logloss: 0.016717\n",
      "[272]\tvalid_0's binary_logloss: 0.0166198\n",
      "[273]\tvalid_0's binary_logloss: 0.0165207\n",
      "[274]\tvalid_0's binary_logloss: 0.0164193\n",
      "[275]\tvalid_0's binary_logloss: 0.0163269\n",
      "[276]\tvalid_0's binary_logloss: 0.0162573\n",
      "[277]\tvalid_0's binary_logloss: 0.0161647\n",
      "[278]\tvalid_0's binary_logloss: 0.0160375\n",
      "[279]\tvalid_0's binary_logloss: 0.0159512\n",
      "[280]\tvalid_0's binary_logloss: 0.0158569\n",
      "[281]\tvalid_0's binary_logloss: 0.0157636\n",
      "[282]\tvalid_0's binary_logloss: 0.0156502\n",
      "[283]\tvalid_0's binary_logloss: 0.0155747\n",
      "[284]\tvalid_0's binary_logloss: 0.0154837\n",
      "[285]\tvalid_0's binary_logloss: 0.0153814\n",
      "[286]\tvalid_0's binary_logloss: 0.0152789\n",
      "[287]\tvalid_0's binary_logloss: 0.0152117\n",
      "[288]\tvalid_0's binary_logloss: 0.0151328\n",
      "[289]\tvalid_0's binary_logloss: 0.015033\n",
      "[290]\tvalid_0's binary_logloss: 0.0149648\n",
      "[291]\tvalid_0's binary_logloss: 0.0148816\n",
      "[292]\tvalid_0's binary_logloss: 0.0147814\n",
      "[293]\tvalid_0's binary_logloss: 0.0146891\n",
      "[294]\tvalid_0's binary_logloss: 0.0146268\n",
      "[295]\tvalid_0's binary_logloss: 0.0145247\n",
      "[296]\tvalid_0's binary_logloss: 0.0144457\n",
      "[297]\tvalid_0's binary_logloss: 0.0143398\n",
      "[298]\tvalid_0's binary_logloss: 0.0142587\n",
      "[299]\tvalid_0's binary_logloss: 0.0141757\n",
      "[300]\tvalid_0's binary_logloss: 0.0141015\n",
      "[301]\tvalid_0's binary_logloss: 0.0140262\n",
      "[302]\tvalid_0's binary_logloss: 0.0139656\n",
      "[303]\tvalid_0's binary_logloss: 0.0139024\n",
      "[304]\tvalid_0's binary_logloss: 0.0138299\n",
      "[305]\tvalid_0's binary_logloss: 0.0137778\n",
      "[306]\tvalid_0's binary_logloss: 0.0136959\n",
      "[307]\tvalid_0's binary_logloss: 0.0136135\n",
      "[308]\tvalid_0's binary_logloss: 0.0135492\n",
      "[309]\tvalid_0's binary_logloss: 0.0134718\n",
      "[310]\tvalid_0's binary_logloss: 0.0133814\n",
      "[311]\tvalid_0's binary_logloss: 0.0133217\n",
      "[312]\tvalid_0's binary_logloss: 0.0132481\n",
      "[313]\tvalid_0's binary_logloss: 0.0131801\n",
      "[314]\tvalid_0's binary_logloss: 0.0131218\n",
      "[315]\tvalid_0's binary_logloss: 0.0130674\n",
      "[316]\tvalid_0's binary_logloss: 0.013004\n",
      "[317]\tvalid_0's binary_logloss: 0.0129179\n",
      "[318]\tvalid_0's binary_logloss: 0.0128665\n",
      "[319]\tvalid_0's binary_logloss: 0.0128138\n",
      "[320]\tvalid_0's binary_logloss: 0.0127469\n",
      "[321]\tvalid_0's binary_logloss: 0.0126707\n",
      "[322]\tvalid_0's binary_logloss: 0.0126235\n",
      "[323]\tvalid_0's binary_logloss: 0.0125614\n",
      "[324]\tvalid_0's binary_logloss: 0.0125207\n",
      "[325]\tvalid_0's binary_logloss: 0.0124602\n",
      "[326]\tvalid_0's binary_logloss: 0.0124081\n",
      "[327]\tvalid_0's binary_logloss: 0.0123551\n",
      "[328]\tvalid_0's binary_logloss: 0.0123034\n",
      "[329]\tvalid_0's binary_logloss: 0.0122361\n",
      "[330]\tvalid_0's binary_logloss: 0.0121754\n",
      "[331]\tvalid_0's binary_logloss: 0.0121429\n",
      "[332]\tvalid_0's binary_logloss: 0.0121066\n",
      "[333]\tvalid_0's binary_logloss: 0.0120444\n",
      "[334]\tvalid_0's binary_logloss: 0.0119943\n",
      "[335]\tvalid_0's binary_logloss: 0.0119373\n",
      "[336]\tvalid_0's binary_logloss: 0.0119015\n",
      "[337]\tvalid_0's binary_logloss: 0.0118628\n",
      "[338]\tvalid_0's binary_logloss: 0.0118163\n",
      "[339]\tvalid_0's binary_logloss: 0.0117614\n",
      "[340]\tvalid_0's binary_logloss: 0.0117099\n",
      "[341]\tvalid_0's binary_logloss: 0.0116536\n",
      "[342]\tvalid_0's binary_logloss: 0.0115939\n",
      "[343]\tvalid_0's binary_logloss: 0.0115504\n",
      "[344]\tvalid_0's binary_logloss: 0.0114972\n",
      "[345]\tvalid_0's binary_logloss: 0.0114322\n",
      "[346]\tvalid_0's binary_logloss: 0.0113828\n",
      "[347]\tvalid_0's binary_logloss: 0.0113398\n",
      "[348]\tvalid_0's binary_logloss: 0.01128\n",
      "[349]\tvalid_0's binary_logloss: 0.0112339\n",
      "[350]\tvalid_0's binary_logloss: 0.0111925\n",
      "[351]\tvalid_0's binary_logloss: 0.0111448\n",
      "[352]\tvalid_0's binary_logloss: 0.0111152\n",
      "[353]\tvalid_0's binary_logloss: 0.0110804\n",
      "[354]\tvalid_0's binary_logloss: 0.0110382\n",
      "[355]\tvalid_0's binary_logloss: 0.0109927\n",
      "[356]\tvalid_0's binary_logloss: 0.0109481\n",
      "[357]\tvalid_0's binary_logloss: 0.0109042\n",
      "[358]\tvalid_0's binary_logloss: 0.0108686\n",
      "[359]\tvalid_0's binary_logloss: 0.0108397\n",
      "[360]\tvalid_0's binary_logloss: 0.0107773\n",
      "[361]\tvalid_0's binary_logloss: 0.010735\n",
      "[362]\tvalid_0's binary_logloss: 0.0106943\n",
      "[363]\tvalid_0's binary_logloss: 0.0106408\n",
      "[364]\tvalid_0's binary_logloss: 0.0106038\n",
      "[365]\tvalid_0's binary_logloss: 0.0105741\n",
      "[366]\tvalid_0's binary_logloss: 0.0105381\n",
      "[367]\tvalid_0's binary_logloss: 0.0104989\n",
      "[368]\tvalid_0's binary_logloss: 0.0104556\n",
      "[369]\tvalid_0's binary_logloss: 0.0104095\n",
      "[370]\tvalid_0's binary_logloss: 0.0103824\n",
      "[371]\tvalid_0's binary_logloss: 0.0103477\n",
      "[372]\tvalid_0's binary_logloss: 0.0103094\n",
      "[373]\tvalid_0's binary_logloss: 0.0102836\n",
      "[374]\tvalid_0's binary_logloss: 0.0102607\n",
      "[375]\tvalid_0's binary_logloss: 0.0102215\n",
      "[376]\tvalid_0's binary_logloss: 0.0101904\n",
      "[377]\tvalid_0's binary_logloss: 0.010152\n",
      "[378]\tvalid_0's binary_logloss: 0.0101117\n",
      "[379]\tvalid_0's binary_logloss: 0.0100644\n",
      "[380]\tvalid_0's binary_logloss: 0.0100413\n",
      "[381]\tvalid_0's binary_logloss: 0.0100016\n",
      "[382]\tvalid_0's binary_logloss: 0.00996274\n",
      "[383]\tvalid_0's binary_logloss: 0.00992908\n",
      "[384]\tvalid_0's binary_logloss: 0.0098931\n",
      "[385]\tvalid_0's binary_logloss: 0.00984794\n",
      "[386]\tvalid_0's binary_logloss: 0.00981395\n",
      "[387]\tvalid_0's binary_logloss: 0.00978042\n",
      "[388]\tvalid_0's binary_logloss: 0.00975467\n",
      "[389]\tvalid_0's binary_logloss: 0.00971252\n",
      "[390]\tvalid_0's binary_logloss: 0.00968537\n",
      "[391]\tvalid_0's binary_logloss: 0.00966036\n",
      "[392]\tvalid_0's binary_logloss: 0.0096348\n",
      "[393]\tvalid_0's binary_logloss: 0.00960343\n",
      "[394]\tvalid_0's binary_logloss: 0.00956844\n",
      "[395]\tvalid_0's binary_logloss: 0.00953895\n",
      "[396]\tvalid_0's binary_logloss: 0.00950994\n",
      "[397]\tvalid_0's binary_logloss: 0.00948416\n",
      "[398]\tvalid_0's binary_logloss: 0.00945099\n",
      "[399]\tvalid_0's binary_logloss: 0.00941679\n",
      "[400]\tvalid_0's binary_logloss: 0.00939505\n",
      "time to train: 12.25s\n"
     ]
    }
   ],
   "source": [
    "lg = lgb.LGBMClassifier(learning_rate=0.1,\n",
    "                        n_estimators=400,\n",
    "                        max_bin=255,\n",
    "                        max_depth=40, \n",
    "                        num_iterations=400, \n",
    "                        random_state=11,\n",
    "                        num_leaves=150,\n",
    "                        silent=False)\n",
    "\n",
    "print(\"training LGBM...\")\n",
    "t0 = time.time()\n",
    "lg.fit(X, y, eval_set=(X_test, y_test), verbose=True) \n",
    "# logloss printed is meaningless, since X includes X_test, just here to print something and show progress\n",
    "print(f\"time to train: {time.time()-t0:.4}s\")\n",
    "\n",
    "# yhat = lg.predict(X_test)\n",
    "# print(\"f1_score:\", f1_score(y_test, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of prediction: 37155\n",
      "submitted!\n"
     ]
    }
   ],
   "source": [
    "upgrade_eval = pd.read_csv(data_folder+\"data/eval/upgrades.csv\")\n",
    "line_ids = upgrade_eval[['line_id']]\n",
    "prediction = line_ids.copy()\n",
    "\n",
    "X_eval = data_eval_extracted.drop(['line_id'], axis=1)\n",
    "prediction['upgrade'] = lg.predict(X_eval)\n",
    "\n",
    "assert(len(prediction) == len(upgrade_eval))\n",
    "print(\"length of prediction:\", len(prediction))\n",
    "\n",
    "submission_path=root_folder+\"code/2021-04-25.csv\"\n",
    "prediction.to_csv(submission_path, header=True, index=None)\n",
    "\n",
    "submission_path=root_folder+\"submission/2021-04-25.csv\"\n",
    "prediction.to_csv(submission_path, header=True, index=None)\n",
    "\n",
    "print(\"submitted!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
