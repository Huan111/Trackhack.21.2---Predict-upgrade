{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==1.0.3 in /opt/conda/lib/python3.7/site-packages (1.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas==1.0.3) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from pandas==1.0.3) (1.19.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas==1.0.3) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas==1.0.3) (1.14.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# installing 1.0.3 because this version of pandas supports write to s3\n",
    "!pip install pandas==1.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "teamname = 'emotional-support-vector-machine-unsw'\n",
    "data_folder='s3://tf-trachack-data/212/'\n",
    "root_folder='s3://tf-trachack-notebooks/'+teamname+'/jupyter/jovyan/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache(obj, filepath):\n",
    "    \"\"\"Cache the result\"\"\"\n",
    "    directory = os.path.dirname(filepath)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    with open(filepath, 'wb+') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def is_cached(filepath):\n",
    "    \"\"\"Checks if the pickle file exists\"\"\"\n",
    "    return os.path.isfile(filepath)\n",
    "\n",
    "\n",
    "def load_cache(filepath):\n",
    "    \"\"\"Load the cached result\"\"\"\n",
    "    assert is_cached(filepath)\n",
    "    \n",
    "    with open(filepath, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_scale(series):\n",
    "    scaler = MinMaxScaler()\n",
    "    return scaler.fit_transform(series.values.reshape(-1,1))\n",
    "\n",
    "def standard_scale(series):\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(series.values.reshape(-1,1))\n",
    "\n",
    "def robust_scale(series):\n",
    "    scaler = RobustScaler()\n",
    "    return scaler.fit_transform(series.values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_datetime(series):\n",
    "    return pd.to_datetime(series, format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lte_downlink = {0:1,1:10,2:50,3:100,4:150,5:300,6:300,7:300,8:3000,9:450,10:450,11:600,12:600,13:390,14:3900,15:800,16:1050,17:25000,18:1200,19:1600,20:2000,21:1400}\n",
    "lte_uplink = {0:1,1:5,2:25,3:50,4:50,5:75,6:50,7:100,8:1500,9:50,10:100,11:50,12:100,13:150,14:1500,15:220,16:100,17:2100,18:210,19:13500,20:315,21:300}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Upgrades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract1(upgrades, filepath=None, overwrite=False):\n",
    "    upgrades = upgrades.drop('date_observed', axis=1)\n",
    "    df1 = upgrades.replace({'no':0,'yes':1})\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Customer Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract2(customer_info, filepath=None, overwrite=False):\n",
    "    customer_info = customer_info.copy()\n",
    "    \n",
    "    df2 = customer_info.copy()\n",
    "    df2 = pd.concat([df2, pd.get_dummies(df2['carrier'], prefix='carrier')], axis=1)\n",
    "\n",
    "    date1 = pd.to_datetime(df2['first_activation_date'], format=\"%Y-%m-%d\")\n",
    "    date2 = pd.to_datetime(df2['redemption_date'], format=\"%Y-%m-%d\")\n",
    "    df2[\"days_to_redemption\"] = np.log((date2 - date1).dt.days+1)\n",
    "    df2['days_to_redemption'].fillna((df2['days_to_redemption'].median()), inplace=True)    \n",
    "    scaler = MinMaxScaler()\n",
    "    df2['days_to_redemption'] = scaler.fit_transform(df2['days_to_redemption'].values.reshape(-1,1))\n",
    "\n",
    "    df2 = pd.concat([\n",
    "        df2,\n",
    "        pd.get_dummies(df2['plan_name'], prefix='plan')\n",
    "    ], axis=1)\n",
    "\n",
    "    df2.drop(\n",
    "        ['carrier', 'first_activation_date', 'plan_subtype', 'redemption_date', 'plan_name'], \n",
    "        axis=1, inplace=True\n",
    "    )\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Phone Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_number(s):\n",
    "    return int(re.search(r'\\d+', str(s)).group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_external_storage_capacity(s):\n",
    "    s = str(s)\n",
    "    if s == 'nan':\n",
    "        return np.nan\n",
    "    if '.' in s:\n",
    "        return float(s)\n",
    "    capacities = np.array(list(map(float, s.split('/'))))\n",
    "    capacity = np.median(capacities)\n",
    "    return capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def extract3(phone_info, upgrades, filepath=None, overwrite=False):\n",
    "    if is_cached(filepath) and not overwrite:\n",
    "        return load_cache(filepath)\n",
    "    \n",
    "    phone_info = phone_info.copy()\n",
    "    \n",
    "    # get has_phone\n",
    "    df3 = phone_info.copy()\n",
    "    df3['has_phone'] = np.where(df3['cpu_cores'].isnull(), 0, 1)\n",
    "\n",
    "    # get total cpus\n",
    "    df3['cpu_cores'] = df3['cpu_cores'].dropna().apply(str).apply(eval)\n",
    "    df3['cpu_cores'].fillna(df3['cpu_cores'].median(), inplace=True)\n",
    "    scaler = MinMaxScaler()\n",
    "    df3['cpu_cores'] = scaler.fit_transform(df3['cpu_cores'].values.reshape(-1,1))\n",
    "    \n",
    "    # expandable storage\n",
    "    df3['expandable_storage'].fillna(0, inplace=True)\n",
    "    \n",
    "    # remove uncommon device types\n",
    "    df3['gsma_device_type'].replace('WLAN Router', 'Other', inplace=True)\n",
    "    df3['gsma_device_type'].replace('Tablet', 'Other', inplace=True)\n",
    "    df3['gsma_device_type'].replace('Modem', 'Other', inplace=True)\n",
    "    df3 = pd.concat([df3, pd.get_dummies(df3['gsma_device_type'], prefix='device_type')], axis=1)\n",
    "    df3.drop(['gsma_device_type', 'device_type_Other'], axis=1, inplace=True)\n",
    "    \n",
    "    df3.drop(['gsma_model_name'], axis=1, inplace=True)\n",
    "    \n",
    "    # operating system\n",
    "    df3['gsma_operating_system'].loc[\n",
    "        (df3['gsma_operating_system'] != 'Android') &\n",
    "        (df3['gsma_operating_system'] != 'iOS') &\n",
    "        (df3['gsma_operating_system'] != 'Linux')\n",
    "    ] = 'Other'\n",
    "    df3 = pd.concat([df3, pd.get_dummies(df3['gsma_operating_system'], prefix='os')], axis=1)\n",
    "    df3.drop(\n",
    "        ['gsma_operating_system', 'os_family', 'os_name', 'os_vendor', 'os_version', \n",
    "         'manufacturer', 'os_Other'], axis=1, inplace=True)\n",
    "    \n",
    "    # internal stroage capacity, use medians\n",
    "    df3['internal_storage_capacity'] = df3['internal_storage_capacity'].apply(split_external_storage_capacity)\n",
    "    df3['internal_storage_capacity'].fillna(df3['internal_storage_capacity'].median(), inplace=True)\n",
    "    df3['internal_storage_capacity'] = np.log2(df3['internal_storage_capacity'])\n",
    "    scaler = StandardScaler()\n",
    "    df3['internal_storage_capacity'] = scaler.fit_transform(df3['internal_storage_capacity'].values.reshape(-1,1))\n",
    "    \n",
    "    df3['lte'].fillna(0, inplace=True)\n",
    "    df3['lte_advanced'].fillna(0, inplace=True)\n",
    "    \n",
    "    df3['lte_category'].fillna(df3['lte_category'].dropna().mode()[0], inplace=True)\n",
    "    df3['lte_downlink'] = df3['lte_category'].map(lte_downlink)\n",
    "    df3['lte_uplink'] = df3['lte_category'].map(lte_uplink)\n",
    "    scaler = RobustScaler()\n",
    "    df3['lte_downlink'] = scaler.fit_transform(df3['lte_downlink'].values.reshape(-1,1))\n",
    "    df3['lte_uplink'] = scaler.fit_transform(df3['lte_uplink'].values.reshape(-1,1))\n",
    "    df3['lte_downlink'] /= 100\n",
    "    df3['lte_uplink'] /= 100\n",
    "    df3.drop(['lte_category'], axis=1, inplace=True)\n",
    "    \n",
    "    df3.drop(['sim_size'], axis=1, inplace=True)\n",
    "    \n",
    "    df3['total_ram'] = df3['total_ram'].apply(split_external_storage_capacity)\n",
    "    df3['total_ram'].fillna(df3['total_ram'].median(), inplace=True)\n",
    "    df3['total_ram'] = np.log2(df3['total_ram'])\n",
    "    scaler = MinMaxScaler()\n",
    "    df3['total_ram'] = scaler.fit_transform(df3['total_ram'].values.reshape(-1,1))\n",
    "    df3['touch_screen'].fillna(0, inplace=True)\n",
    "    df3['wi_fi'].fillna(0, inplace=True)\n",
    "        \n",
    "    # years since release\n",
    "    df3 = pd.merge(df3, upgrades[['line_id', 'date_observed']].copy(), on='line_id')\n",
    "    df3['date_observed'] = pd.to_datetime(df3['date_observed'], format=\"%Y-%m-%d\").dt.year\n",
    "    \n",
    "    df3['years_since_release'] = df3['date_observed'] - df3['year_released']\n",
    "    df3['years_since_release'].fillna(df3['years_since_release'].dropna().median(), inplace=True)\n",
    "    df3['years_since_release'] = minmax_scale(df3['years_since_release'])\n",
    "    \n",
    "    df3.drop(['date_observed', 'year_released'], axis=1, inplace=True)\n",
    "    \n",
    "    cache(df3, filepath)\n",
    "    \n",
    "    return df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Redemptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract4(redemptions, upgrades, filepath=None, overwrite=False):\n",
    "    if is_cached(filepath) and not overwrite:\n",
    "        return load_cache(filepath)\n",
    "    \n",
    "    redemptions = redemptions.copy()\n",
    "    df4 = upgrades[['line_id']].copy()\n",
    "    \n",
    "    #num redemptions\n",
    "    num_redemptions = redemptions.groupby(['line_id']).size().to_frame('num_redemptions')\n",
    "    df4 = pd.merge(df4, num_redemptions, on='line_id', how='left')\n",
    "    df4['num_redemptions'].fillna(0, inplace=True)\n",
    "    df4['num_redemptions'] = np.log(df4['num_redemptions']+1)\n",
    "    scaler = MinMaxScaler()\n",
    "    df4['num_redemptions'] = scaler.fit_transform(df4['num_redemptions'].values.reshape(-1,1))\n",
    "    \n",
    "    # num one offs\n",
    "    one_off_threshold = 15\n",
    "    num_one_offs = redemptions[redemptions['gross_revenue'] <= one_off_threshold].groupby(['line_id']).size().to_frame('num_one_offs')\n",
    "    df4 = pd.merge(df4, num_one_offs, on='line_id', how='left')\n",
    "    df4['num_one_offs'].fillna(0, inplace=True)\n",
    "    df4['num_one_offs'] = np.log(df4['num_one_offs']+1)\n",
    "    scaler = MinMaxScaler()\n",
    "    df4['num_one_offs'] = scaler.fit_transform(df4['num_one_offs'].values.reshape(-1,1))\n",
    "    \n",
    "    # total spent\n",
    "    total_spent = redemptions.groupby(['line_id'])['gross_revenue'].sum().to_frame('total_spent')\n",
    "    df4 = pd.merge(df4, total_spent, on='line_id', how='left')\n",
    "    df4['total_spent'].fillna(0, inplace=True)\n",
    "    \n",
    "    # average monthly\n",
    "    redemptions['redemption_date'] = pd.to_datetime(redemptions['redemption_date'], format=\"%Y-%m-%d\")\n",
    "    date_start = redemptions.groupby(['line_id'])['redemption_date'].min().to_frame('date_start')\n",
    "    df4 = pd.merge(df4, date_start, on='line_id', how='left')\n",
    "    date_end = redemptions.groupby(['line_id'])['redemption_date'].max().to_frame('date_end')\n",
    "    df4 = pd.merge(df4, date_end, on='line_id', how='left')\n",
    "    df4['date_start'].fillna(np.datetime64('1970'), inplace=True)\n",
    "    df4['date_end'].fillna(np.datetime64('1970'), inplace=True)\n",
    "    \n",
    "    df4['total_months'] = (df4['date_end'] - df4['date_start']).dt.days//30+1\n",
    "    df4['average_monthly_spend'] = df4['total_spent'] / df4['total_months']\n",
    "    df4.drop(['date_start', 'date_end', 'total_months'], axis=1, inplace=True)\n",
    "    \n",
    "    df4['total_spent'] = np.log(df4['total_spent']+1)\n",
    "    scaler = StandardScaler()\n",
    "    df4['total_spent'] = scaler.fit_transform(df4['total_spent'].values.reshape(-1,1))\n",
    "    \n",
    "    df4['average_monthly_spend'] = np.log(df4['average_monthly_spend']+1)\n",
    "    scaler = MinMaxScaler()\n",
    "    df4['average_monthly_spend'] = scaler.fit_transform(df4['average_monthly_spend'].values.reshape(-1,1))\n",
    "    \n",
    "    # monthly plan cost\n",
    "    monthly_plan_cost = redemptions[redemptions['gross_revenue'] > one_off_threshold].groupby(['line_id']).median()\n",
    "    monthly_plan_cost = monthly_plan_cost.rename(columns={'gross_revenue':'monthly_plan_cost'})\n",
    "        \n",
    "    df4 = pd.merge(df4, monthly_plan_cost, on='line_id', how='left')\n",
    "    df4['monthly_plan_cost'].fillna(0, inplace=True)\n",
    "    df4['monthly_plan_cost'] = np.log(df4['num_one_offs']+1)\n",
    "    scaler = MinMaxScaler()\n",
    "    df4['monthly_plan_cost'] = scaler.fit_transform(df4['num_one_offs'].values.reshape(-1,1))\n",
    "    \n",
    "    cache(df4, filepath)\n",
    "    \n",
    "    return df4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Deactivations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract5(deactivations, upgrades, filepath=None, overwrite=False):\n",
    "    if is_cached(filepath) and not overwrite:\n",
    "        return load_cache(filepath)\n",
    "    \n",
    "    deactivations = deactivations.copy()\n",
    "    df5 = upgrades[['line_id']].copy()\n",
    "    \n",
    "    # has deactivated\n",
    "    has_deactivated = deactivations.groupby('line_id')['deactivation_reason'].any()\n",
    "    has_deactivated = has_deactivated.rename(f'has_deactivated')\n",
    "    df5 = pd.merge(df5, has_deactivated, on='line_id', how='left')\n",
    "    df5[f'has_deactivated'].replace({np.nan:0, True:1}, inplace=True)\n",
    "    \n",
    "    # has certain deactivation reasons\n",
    "    useful_values = [\"PASTDUE\", \"UPGRADE\", \"DEVICE CHANGE INQUIRY\", \"STOLEN\", \n",
    "                    \"DEFECTIVE\", \"ACTIVE UPGRADE\", \"REFURBISHED\", \"NO NEED OF PHONE\", \"DEVICERETURN\"]\n",
    "    for useful_value in useful_values: \n",
    "        has_value = deactivations[deactivations['deactivation_reason'] == useful_value].groupby('line_id')['deactivation_reason'].any()\n",
    "        has_value = has_value.rename(f'reason_{useful_value}')\n",
    "        df5 = pd.merge(df5, has_value, on='line_id', how='left')\n",
    "        df5[f'reason_{useful_value}'].replace({np.nan:0, True:1}, inplace=True)\n",
    "\n",
    "    # num deactivations\n",
    "    num_deactivations = deactivations.groupby(['line_id']).size().to_frame('num_deactivations')\n",
    "    df5 = pd.merge(df5, num_deactivations, on='line_id', how='left')\n",
    "    df5['num_deactivations'].fillna(0, inplace=True)\n",
    "    df5['num_deactivations'] = np.log(df5['num_deactivations']+1)\n",
    "    df5['num_deactivations'] = minmax_scale(df5['num_deactivations'])\n",
    "        \n",
    "    cache(df5, filepath)\n",
    "        \n",
    "    return df5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Reactivations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract6(reactivations, deactivations, upgrades, filepath=None, overwrite=False):\n",
    "    if is_cached(filepath) and not overwrite:\n",
    "        return load_cache(filepath)\n",
    "    \n",
    "    reactivations = reactivations.copy()\n",
    "    df6 = upgrades[['line_id']].copy()\n",
    "    \n",
    "    # check if active\n",
    "    df6['is_active'] = 1\n",
    "    \n",
    "    deactivations['deactivation_date'] = pd.to_datetime(deactivations['deactivation_date'], format=\"%Y-%m-%d\")\n",
    "    reactivations['reactivation_date'] = pd.to_datetime(reactivations['reactivation_date'], format=\"%Y-%m-%d\")\n",
    "    \n",
    "    last_deactivate = deactivations.groupby(['line_id'])['deactivation_date'].max().to_frame('last_deactivate')\n",
    "    last_reactivate = reactivations.groupby(['line_id'])['reactivation_date'].max().to_frame('last_reactivate')\n",
    "    \n",
    "    df6 = pd.merge(df6, last_deactivate, on='line_id', how='left')\n",
    "    df6 = pd.merge(df6, last_reactivate, on='line_id', how='left')\n",
    "    \n",
    "    has_deactivated = deactivations.groupby('line_id')['deactivation_reason'].any()\n",
    "    has_deactivated = has_deactivated.rename(f'has_deactivated')\n",
    "    df6 = pd.merge(df6, has_deactivated, on='line_id', how='left')\n",
    "    df6['has_deactivated'].replace({np.nan:0, True:1}, inplace=True)\n",
    "    df6['reactivated'] = np.where(df6['last_reactivate'] - df6['last_deactivate'] >= np.timedelta64(0, 'D'), 1, 0)\n",
    "    \n",
    "    df6['is_active'] = 1 - df6[f'has_deactivated'] + df6['reactivated']\n",
    "    \n",
    "    df6.drop(['last_deactivate', 'has_deactivated', 'reactivated'], axis=1, inplace=True)\n",
    "    \n",
    "    #num reactivations\n",
    "    num_reactivations = reactivations.groupby(['line_id']).size().to_frame('num_reactivations')\n",
    "    df6 = pd.merge(df6, num_reactivations, on='line_id', how='left')\n",
    "    df6['num_reactivations'].fillna(0, inplace=True)\n",
    "    df6['num_reactivations'] = np.log(df6['num_reactivations']+1)\n",
    "    df6['num_reactivations'] = minmax_scale(df6['num_reactivations'])\n",
    "    \n",
    "    # days since last reactivation\n",
    "    df6 = pd.merge(df6, upgrades[['line_id','date_observed']], on='line_id', how='left')\n",
    "    \n",
    "    df6['date_observed'] = convert_to_datetime(df6['date_observed'])\n",
    "    df6['days_since_reactivation'] = (df6['date_observed'] - df6['last_reactivate']).dt.days\n",
    "    df6['days_since_reactivation'] = minmax_scale(np.log(df6['days_since_reactivation']+1))\n",
    "    df6['days_since_reactivation'].fillna(0, inplace=True)\n",
    "    \n",
    "    df6.drop(['date_observed', 'last_reactivate'], axis=1, inplace=True)\n",
    "        \n",
    "    cache(df6, filepath)\n",
    "    \n",
    "    return df6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Suspensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract7(suspensions, upgrades, filepath=None, overwrite=False):\n",
    "    if is_cached(filepath) and not overwrite:\n",
    "        return load_cache(filepath)\n",
    "    \n",
    "    suspensions = suspensions.copy()\n",
    "    df7 = upgrades[['line_id', 'date_observed']].copy()\n",
    "    \n",
    "    # num and has been suspended\n",
    "    num_suspensions = suspensions.groupby(['line_id']).size().to_frame('num_suspensions')\n",
    "    df7 = pd.merge(df7, num_suspensions, on='line_id', how='left');\n",
    "    df7['num_suspensions'].fillna(0, inplace=True)\n",
    "    df7['has_been_suspended'] = np.where(df7['num_suspensions'] > 0, 1, 0)\n",
    "    df7['num_suspensions'] = minmax_scale(np.log(df7['num_suspensions']+1))\n",
    "    \n",
    "    # months since last suspension\n",
    "\n",
    "    df7['date_observed'] = convert_to_datetime(df7['date_observed'])\n",
    "    suspensions['suspension_start_date'] = convert_to_datetime(suspensions['suspension_start_date'])\n",
    "    suspensions['suspension_end_date'] = convert_to_datetime(suspensions['suspension_end_date'])\n",
    "\n",
    "    last_suspension_date = suspensions.groupby('line_id')['suspension_end_date'].max()\n",
    "    last_suspension_date.rename('last_suspension_date')\n",
    "    df7 = pd.merge(df7, last_suspension_date, on='line_id', how='left')\n",
    "    \n",
    "    df7['months_since_suspended'] = (df7['date_observed'] - df7['suspension_end_date']).dt.days/30+1\n",
    "    df7['months_since_suspended'].fillna(0, inplace=True)\n",
    "    df7['months_since_suspended'] = minmax_scale(np.log(df7['months_since_suspended']+1))\n",
    "    \n",
    "    df7.drop(['date_observed', 'suspension_end_date'], axis=1, inplace=True)\n",
    "    \n",
    "    # average suspension length\n",
    "    suspensions['suspension_length'] = (suspensions['suspension_end_date'] - suspensions['suspension_start_date']).dt.days\n",
    "    average_suspension_length = suspensions.groupby('line_id')['suspension_length'].mean().to_frame('average_suspension_length')\n",
    "    df7 = pd.merge(df7, average_suspension_length, on='line_id', how='left')\n",
    "    df7['average_suspension_length'].fillna(0, inplace=True)\n",
    "    df7['average_suspension_length'] = minmax_scale(df7['average_suspension_length']+1)\n",
    "    \n",
    "    cache(df7, filepath)\n",
    "    return df7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Network Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract8(network_usage, upgrades, filepath=None, overwrite=False):\n",
    "    if is_cached(filepath) and not overwrite:\n",
    "        return load_cache(filepath)\n",
    "    \n",
    "    network_usage = network_usage.copy()\n",
    "    df8 = upgrades[['line_id']].copy()\n",
    "    \n",
    "    # sms/mms in and out\n",
    "    print(\"  sms and mms...\")\n",
    "    sms_in_total = (network_usage.groupby('line_id')['mms_in'].sum() +\n",
    "         network_usage.groupby('line_id')['sms_in'].sum()).to_frame('sms_in_total')\n",
    "    df8 = pd.merge(df8, sms_in_total, on='line_id', how='left')\n",
    "    df8['sms_in_total'].fillna(0, inplace=True)    \n",
    "    \n",
    "    sms_out_total = (network_usage.groupby('line_id')['mms_out'].sum() +\n",
    "         network_usage.groupby('line_id')['sms_out'].sum()).to_frame('sms_out_total')\n",
    "    df8 = pd.merge(df8, sms_out_total, on='line_id', how='left')\n",
    "    df8['sms_out_total'].fillna(0, inplace=True)  \n",
    "    \n",
    "#     df8['sms_diff'] = df8['sms_in_total'] - df8['sms_out_total']\n",
    "    df8['sms_total'] = df8['sms_in_total'] + df8['sms_out_total']\n",
    "    \n",
    "    df8['over_15_sms'] = np.choose(df8['sms_total'] >= 15, [0, 1])\n",
    "    \n",
    "    # kilobytes of data\n",
    "    print(\"  kilobytes of data...\")\n",
    "    hotspot_kb = network_usage.groupby('line_id')['hotspot_kb'].sum()\n",
    "    df8 = pd.merge(df8, hotspot_kb, on='line_id', how='left')\n",
    "    df8['hotspot_kb'].fillna(0, inplace=True)  \n",
    "    df8['hotspot_gb'] = df8['hotspot_kb'] / 1024.0 / 1024.0\n",
    "    \n",
    "    kb_5g = network_usage.groupby('line_id')['kb_5g'].sum()\n",
    "    df8 = pd.merge(df8, kb_5g, on='line_id', how='left')\n",
    "    df8['kb_5g'].fillna(0, inplace=True) \n",
    "    df8['gb_5g'] = df8['kb_5g'] / 1024.0 / 1024.0\n",
    "             \n",
    "    total_kb = network_usage.groupby('line_id')['total_kb'].sum()\n",
    "    df8 = pd.merge(df8, total_kb, on='line_id', how='left')\n",
    "    df8['total_kb'].fillna(0, inplace=True)  \n",
    "    df8['total_gb'] = df8['total_kb'] / 1024.0 / 1024.0\n",
    "    \n",
    "    df8.drop(['hotspot_kb', 'kb_5g', 'total_kb'], axis=1, inplace=True)\n",
    "    \n",
    "    # voice counts\n",
    "    print(\"  voice counts...\")\n",
    "    voice_count_in = network_usage.groupby('line_id')['voice_count_in'].sum()\n",
    "    df8 = pd.merge(df8, voice_count_in, on='line_id', how='left')\n",
    "    df8['voice_count_in'].fillna(0, inplace=True) \n",
    "    \n",
    "    voice_count_total = network_usage.groupby('line_id')['voice_count_total'].sum()\n",
    "    df8 = pd.merge(df8, voice_count_total, on='line_id', how='left')\n",
    "    df8['voice_count_total'].fillna(0, inplace=True) \n",
    "    \n",
    "    df8['voice_count_out'] = df8['voice_count_total'] - df8['voice_count_in']\n",
    "#     df8['voice_count_diff'] = df8['voice_count_in'] - df8['voice_count_out']  \n",
    "        \n",
    "    df8['over_5_voice'] = np.choose(df8['voice_count_total'] >= 5, [0, 1])\n",
    "        \n",
    "    # voice minutes\n",
    "    print(\"  voice minutes..\")\n",
    "    voice_min_in = network_usage.groupby('line_id')['voice_min_in'].sum()\n",
    "    df8 = pd.merge(df8, voice_min_in, on='line_id', how='left')\n",
    "    df8['voice_min_in'].fillna(0, inplace=True) \n",
    "    \n",
    "    voice_min_out = network_usage.groupby('line_id')['voice_min_out'].sum()\n",
    "    df8 = pd.merge(df8, voice_min_out, on='line_id', how='left')\n",
    "    df8['voice_min_out'].fillna(0, inplace=True) \n",
    "    \n",
    "    df8['voice_min_total'] = df8['voice_min_in'] + df8['voice_min_out']\n",
    "    \n",
    "    # average call length\n",
    "    df8['average_call_length'] = np.choose(\n",
    "        df8['voice_count_total'] == 0, \n",
    "        [df8['voice_min_total'] / df8['voice_count_total'], 0]\n",
    "    )\n",
    "    \n",
    "    #num usages\n",
    "    num_network_usages = network_usage.groupby(['line_id']).size().to_frame('num_network_usages')\n",
    "    df8 = pd.merge(df8, num_network_usages, on='line_id', how='left')\n",
    "    df8['num_network_usages'].fillna(0, inplace=True)\n",
    "    df8['num_network_usages'] = np.log(df8['num_network_usages']+1)\n",
    "    df8['num_network_usages'] = minmax_scale(df8['num_network_usages'])\n",
    "    \n",
    "    print(\"  scaling all parameters..\")\n",
    "    df8['sms_in_total'] = standard_scale(np.log(df8['sms_in_total']+1))\n",
    "    df8['sms_out_total'] = standard_scale(np.log(df8['sms_out_total']+1))\n",
    "    df8['sms_total'] = standard_scale(np.log(df8['sms_total']+1))\n",
    "#     df8['sms_diff'] = standard_scale(df8['sms_diff'])\n",
    "    df8['hotspot_gb'] = standard_scale(np.log(df8['hotspot_gb']+1))\n",
    "    df8['gb_5g'] = standard_scale(np.log(df8['gb_5g']+1))\n",
    "    df8['total_gb'] = standard_scale(np.log(df8['total_gb']+1))\n",
    "    df8['voice_count_in'] = standard_scale(np.log(df8['voice_count_in']+1))\n",
    "    df8['voice_count_out'] = standard_scale(np.log(df8['voice_count_out']+1))\n",
    "    df8['voice_count_total'] = standard_scale(np.log(df8['voice_count_total']+1))\n",
    "    df8['voice_min_in'] = standard_scale(np.log(df8['voice_min_in']+1))\n",
    "    df8['voice_min_out'] = standard_scale(np.log(df8['voice_min_out']+1))\n",
    "    df8['voice_min_total'] = standard_scale(np.log(df8['voice_min_total']+1))\n",
    "#     df8['voice_count_diff'] = standard_scale(df8['voice_count_diff'])\n",
    "    df8['average_call_length'] = standard_scale(np.log(df8['average_call_length']+1))\n",
    "    \n",
    "    cache(df8, filepath)\n",
    "    \n",
    "    return df8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. LRP Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract9(lrp_points, upgrades, filepath=None, overwrite=False):\n",
    "    if is_cached(filepath) and not overwrite:\n",
    "        return load_cache(filepath)\n",
    "    \n",
    "    lrp_points = lrp_points.copy()\n",
    "    df9 = upgrades[['line_id']].copy()\n",
    "    \n",
    "    df9 = pd.merge(df9, lrp_points, on='line_id', how='left')\n",
    "    \n",
    "    # total_quantity\n",
    "    df9['total_quantity'].fillna(0, inplace=True)\n",
    "\n",
    "    # has_earned_points\n",
    "    df9['has_earned_points'] = np.where(df9['status'] == 'ENROLLED', 1, 0)\n",
    "    \n",
    "    # total_spent\n",
    "    df9['total_spent_lrp'] = df9['total_quantity'] - df9['quantity']\n",
    "    df9['total_spent_lrp'].fillna(0, inplace=True)\n",
    "    \n",
    "    # has_spent\n",
    "    df9['has_spent'] = np.where(df9['total_spent_lrp'] > 0, 1, 0)\n",
    "    \n",
    "    # days_since_earnt\n",
    "    df9['update_date'] = convert_to_datetime(df9['update_date'])\n",
    "    df9 = pd.merge(df9, upgrades[['line_id', 'date_observed']].copy(), on='line_id', how='left')\n",
    "    df9['date_observed'] = convert_to_datetime(df9['date_observed'])\n",
    "    \n",
    "    df9['days_since_update'] = (df9['date_observed'] - df9['update_date']).dt.days\n",
    "    df9['days_since_update'].fillna(0, inplace=True)\n",
    "    df9['days_since_update'] = minmax_scale(np.log(df9['days_since_update']+1))\n",
    "    \n",
    "    df9.drop(['quantity', 'status', 'update_date', 'date_observed'], axis=1, inplace=True)\n",
    "\n",
    "    cache(df9, filepath)\n",
    "    return df9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. LRP Enrollment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract10(lrp_enrollment, upgrades, filepath=None, overwrite=False):\n",
    "    if is_cached(filepath) and not overwrite:\n",
    "        return load_cache(filepath)\n",
    "    \n",
    "    lrp_enrollment = lrp_enrollment.copy()\n",
    "    df10 = upgrades[['line_id', 'date_observed']].copy()\n",
    "    \n",
    "    num_enrols = lrp_enrollment.groupby('line_id').size().to_frame('num_enrols')\n",
    "    df10 = pd.merge(df10, num_enrols, on='line_id', how='left')\n",
    "    df10['num_enrols'].fillna(0, inplace=True)\n",
    "    df10['has_enrolled'] = np.where(df10['num_enrols'] > 0, 1, 0)\n",
    "    df10['has_enrolled_over_twice'] = np.where(df10['num_enrols'] >= 2, 1, 0)\n",
    "    \n",
    "    df10['num_enrols'] = minmax_scale(df10['num_enrols'])\n",
    "    \n",
    "    # num months since enrolment\n",
    "    df10['date_observed'] = convert_to_datetime(df10['date_observed'])\n",
    "    lrp_enrollment['lrp_enrollment_date'] = convert_to_datetime(lrp_enrollment['lrp_enrollment_date'])\n",
    "    \n",
    "    last_enrolled_date = lrp_enrollment.groupby('line_id')['lrp_enrollment_date'].max()\n",
    "    df10 = pd.merge(df10, last_enrolled_date, on='line_id', how='left')\n",
    "    \n",
    "    df10['months_since_enrolled'] = (df10['date_observed'] - df10['lrp_enrollment_date']).dt.days/30+1\n",
    "    df10['months_since_enrolled'].fillna(0, inplace=True)\n",
    "    df10['months_since_enrolled'] = standard_scale(np.log(df10['months_since_enrolled']+1))\n",
    "    \n",
    "    df10.drop(['date_observed', 'lrp_enrollment_date'], axis=1, inplace=True)\n",
    "    \n",
    "    cache(df10, filepath)\n",
    "    return df10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data=\"dev\"):\n",
    "    assert(data == \"dev\" or data == \"eval\")\n",
    "    \n",
    "    if data == \"dev\":\n",
    "        base_folder = data_folder+\"data/dev/\"\n",
    "    elif data == \"eval\":\n",
    "        base_folder = data_folder+\"data/eval/\"\n",
    "        \n",
    "    print(\"loading upgrades...\")\n",
    "    upgrades = pd.read_csv(base_folder + \"upgrades.csv\")\n",
    "    print(\"loading customer_info...\")\n",
    "    customer_info = pd.read_csv(base_folder + \"customer_info.csv\")\n",
    "    print(\"loading phone_info...\")\n",
    "    phone_info = pd.read_csv(base_folder + \"phone_info.csv\")\n",
    "    print(\"loading redemptions...\")\n",
    "    redemptions = pd.read_csv(base_folder + \"redemptions.csv\")\n",
    "    print(\"loading deactivations...\")\n",
    "    deactivations = pd.read_csv(base_folder + \"deactivations.csv\")\n",
    "    print(\"loading reactivations...\")\n",
    "    reactivations = pd.read_csv(base_folder + \"reactivations.csv\")\n",
    "    print(\"loading suspensions...\")\n",
    "    suspensions = pd.read_csv(base_folder + \"suspensions.csv\")\n",
    "    print(\"loading network_usage_domestic...\")\n",
    "    network_usage_domestic = pd.read_csv(base_folder + \"network_usage_domestic.csv\")\n",
    "    print(\"loading lrp_points...\")\n",
    "    lrp_points = pd.read_csv(base_folder + \"lrp_points.csv\")\n",
    "    print(\"loading lrp_enrollment...\")\n",
    "    lrp_enrollment = pd.read_csv(base_folder + \"lrp_enrollment.csv\")\n",
    "    \n",
    "    print(\"done!\")\n",
    "    \n",
    "    return upgrades, customer_info, phone_info, redemptions, deactivations, reactivations, \\\n",
    "        suspensions, network_usage_domestic, lrp_points, lrp_enrollment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(datasets, data=\"dev\", overwrite=False):\n",
    "    assert(data == \"dev\" or data == \"eval\")\n",
    "    \n",
    "    if data == \"dev\":\n",
    "        base_path = root_folder+\"kevin-zhu/pickles/{}_dev.pickle\"\n",
    "        output_path = root_folder+\"kevin-zhu/dev-extracted.csv\"\n",
    "        \n",
    "    elif data == \"eval\":\n",
    "        base_path = root_folder+\"kevin-zhu/pickles/{}_eval.pickle\"\n",
    "        output_path = root_folder+\"kevin-zhu/dev-extracted.csv\"\n",
    "        \n",
    "    (upgrades, customer_info, phone_info, redemptions, deactivations, reactivations,\n",
    "        suspensions, network_usage_domestic, lrp_points, lrp_enrollment) = datasets\n",
    "    \n",
    "    print(\"extracting 1...\")\n",
    "    df1 = extract1(upgrades)\n",
    "    print(\"extracting 2...\")\n",
    "    df2 = extract2(customer_info, upgrades)\n",
    "    print(\"extracting 3...\")\n",
    "    df3 = extract3(phone_info, upgrades, filepath=base_path.format(3), overwrite=overwrite)\n",
    "    print(\"extracting 4...\")\n",
    "    df4 = extract4(redemptions, upgrades, filepath=base_path.format(4), overwrite=overwrite)\n",
    "    print(\"extracting 5...\")\n",
    "    df5 = extract5(deactivations, upgrades, filepath=base_path.format(5), overwrite=overwrite)\n",
    "    print(\"extracting 6...\")\n",
    "    df6 = extract6(reactivations, deactivations, upgrades, filepath=base_path.format(6), overwrite=overwrite)\n",
    "    print(\"extracting 7...\")\n",
    "    df7 = extract7(suspensions, upgrades, filepath=base_path.format(7), overwrite=overwrite)\n",
    "    print(\"extracting 8...\")\n",
    "    df8 = extract8(network_usage_domestic, upgrades, filepath=base_path.format(8), overwrite=overwrite)\n",
    "    print(\"extracting 9...\")\n",
    "    df9 = extract9(lrp_points, upgrades, filepath=base_path.format(9), overwrite=overwrite)\n",
    "    print(\"extracting 10...\")\n",
    "    df10 = extract10(lrp_enrollment, upgrades, filepath=base_path.format(10), overwrite=overwrite)\n",
    "    \n",
    "    print(\"extracted all features\")\n",
    "    \n",
    "    print(\"merging them all together...\")\n",
    "    dfs = [df1, df2, df3, df4, df5, df6, df7, df8, df9, df10]\n",
    "    df = dfs[0].copy()\n",
    "    for i in range(1, len(dfs)):\n",
    "        assert(len(dfs[i]) == len(df))\n",
    "        df = pd.merge(df, dfs[i], on='line_id')\n",
    "    \n",
    "    assert(df.isna().any().any() == False)\n",
    "        \n",
    "    print(\"saving to file...\")\n",
    "    print(output_path)\n",
    "    df.to_csv(output_path, header=True, index=None)\n",
    "    \n",
    "    print(\"done!\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting 1...\n",
      "extracting 2...\n",
      "extracting 3...\n",
      "extracting 4...\n",
      "extracting 5...\n",
      "extracting 6...\n",
      "extracting 7...\n",
      "extracting 8...\n",
      "  sms and mms...\n",
      "  kilobytes of data...\n",
      "  voice counts...\n",
      "  voice minutes..\n",
      "  scaling all parameters..\n",
      "extracting 9...\n",
      "extracting 10...\n",
      "extracted all features\n",
      "merging them all together...\n",
      "saving to file...\n",
      "s3://tf-trachack-notebooks/emotional-support-vector-machine-unsw/jupyter/jovyan/kevin-zhu/dev-extracted.csv\n",
      "done!\n",
      "number of features: 77\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    data_dev\n",
    "except NameError:\n",
    "    # generate data if it has not been generated yet\n",
    "    data_dev = load_data('dev')\n",
    "    data_eval = load_data('eval')\n",
    "\n",
    "df = extract_features(data_dev, 'dev', overwrite=True)\n",
    "print(\"number of features:\", len(df.dtypes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(data_eval, clf, filename, submit=False):\n",
    "    upgrades = pd.read_csv(data_folder+\"data/eval/upgrades.csv\")\n",
    "    \n",
    "    extract_features(data_eval, 'eval', overwrite=True)\n",
    "    data_eval = pd.read_csv(root_folder+\"kevin-zhu/eval-extracted.csv\")\n",
    "    X_eval = data_eval.drop(['line_id'], axis=1)\n",
    "    \n",
    "    prediction = upgrades_eval[['line_id']].copy()\n",
    "    prediction['upgrade'] = clf.predict(X_eval)\n",
    "\n",
    "    assert(len(prediction) == len(upgrades_eval))\n",
    "    print(len(prediction))\n",
    "    \n",
    "    submission_path=root_folder+\"kevin-zhu/submission/\" + filename\n",
    "    prediction.to_csv(submission_path,header=True,index=None)\n",
    "    \n",
    "    if submit:\n",
    "        submission_path=root_folder+\"submission/\" + filename\n",
    "        prediction.to_csv(submission_path,header=True,index=None)\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num features: 75\n",
      "num train samples: 44694\n",
      "num validation samples: 11174\n",
      "num test samples: 11174\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(root_folder+\"kevin-zhu/dev-extracted.csv\")\n",
    "y = data['upgrade']\n",
    "X = data.drop(['line_id', 'upgrade'], axis = 1)\n",
    "\n",
    "print(\"num features:\", len(X.dtypes))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=0)\n",
    "print(\"num train samples:\", len(X_train))\n",
    "print(\"num validation samples:\", len(X_val))\n",
    "print(\"num test samples:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html>\n",
    "\n",
    "<https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training LGBM...\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 12200, number of negative: 32494\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.309302 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5916\n",
      "[LightGBM] [Info] Number of data points in the train set: 44694, number of used features: 69\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.272967 -> initscore=-0.979620\n",
      "[LightGBM] [Info] Start training from score -0.979620\n",
      "[1]\tvalid_0's binary_logloss: 0.516577\n",
      "[2]\tvalid_0's binary_logloss: 0.467335\n",
      "[3]\tvalid_0's binary_logloss: 0.42784\n",
      "[4]\tvalid_0's binary_logloss: 0.395623\n",
      "[5]\tvalid_0's binary_logloss: 0.3688\n",
      "[6]\tvalid_0's binary_logloss: 0.345681\n",
      "[7]\tvalid_0's binary_logloss: 0.325841\n",
      "[8]\tvalid_0's binary_logloss: 0.308439\n",
      "[9]\tvalid_0's binary_logloss: 0.293643\n",
      "[10]\tvalid_0's binary_logloss: 0.280687\n",
      "[11]\tvalid_0's binary_logloss: 0.269344\n",
      "[12]\tvalid_0's binary_logloss: 0.259021\n",
      "[13]\tvalid_0's binary_logloss: 0.249737\n",
      "[14]\tvalid_0's binary_logloss: 0.241849\n",
      "[15]\tvalid_0's binary_logloss: 0.234815\n",
      "[16]\tvalid_0's binary_logloss: 0.228266\n",
      "[17]\tvalid_0's binary_logloss: 0.222151\n",
      "[18]\tvalid_0's binary_logloss: 0.216616\n",
      "[19]\tvalid_0's binary_logloss: 0.211875\n",
      "[20]\tvalid_0's binary_logloss: 0.207233\n",
      "[21]\tvalid_0's binary_logloss: 0.20355\n",
      "[22]\tvalid_0's binary_logloss: 0.200058\n",
      "[23]\tvalid_0's binary_logloss: 0.19681\n",
      "[24]\tvalid_0's binary_logloss: 0.193918\n",
      "[25]\tvalid_0's binary_logloss: 0.191175\n",
      "[26]\tvalid_0's binary_logloss: 0.188455\n",
      "[27]\tvalid_0's binary_logloss: 0.18623\n",
      "[28]\tvalid_0's binary_logloss: 0.183957\n",
      "[29]\tvalid_0's binary_logloss: 0.181975\n",
      "[30]\tvalid_0's binary_logloss: 0.180227\n",
      "[31]\tvalid_0's binary_logloss: 0.178561\n",
      "[32]\tvalid_0's binary_logloss: 0.176731\n",
      "[33]\tvalid_0's binary_logloss: 0.175581\n",
      "[34]\tvalid_0's binary_logloss: 0.17409\n",
      "[35]\tvalid_0's binary_logloss: 0.172611\n",
      "[36]\tvalid_0's binary_logloss: 0.171614\n",
      "[37]\tvalid_0's binary_logloss: 0.17024\n",
      "[38]\tvalid_0's binary_logloss: 0.169254\n",
      "[39]\tvalid_0's binary_logloss: 0.168119\n",
      "[40]\tvalid_0's binary_logloss: 0.167238\n",
      "[41]\tvalid_0's binary_logloss: 0.166415\n",
      "[42]\tvalid_0's binary_logloss: 0.165171\n",
      "[43]\tvalid_0's binary_logloss: 0.164277\n",
      "[44]\tvalid_0's binary_logloss: 0.163463\n",
      "[45]\tvalid_0's binary_logloss: 0.162636\n",
      "[46]\tvalid_0's binary_logloss: 0.162071\n",
      "[47]\tvalid_0's binary_logloss: 0.161168\n",
      "[48]\tvalid_0's binary_logloss: 0.160524\n",
      "[49]\tvalid_0's binary_logloss: 0.160135\n",
      "[50]\tvalid_0's binary_logloss: 0.159802\n",
      "[51]\tvalid_0's binary_logloss: 0.159319\n",
      "[52]\tvalid_0's binary_logloss: 0.158825\n",
      "[53]\tvalid_0's binary_logloss: 0.158384\n",
      "[54]\tvalid_0's binary_logloss: 0.158168\n",
      "[55]\tvalid_0's binary_logloss: 0.157658\n",
      "[56]\tvalid_0's binary_logloss: 0.157355\n",
      "[57]\tvalid_0's binary_logloss: 0.156933\n",
      "[58]\tvalid_0's binary_logloss: 0.156564\n",
      "[59]\tvalid_0's binary_logloss: 0.156137\n",
      "[60]\tvalid_0's binary_logloss: 0.155374\n",
      "[61]\tvalid_0's binary_logloss: 0.154912\n",
      "[62]\tvalid_0's binary_logloss: 0.154488\n",
      "[63]\tvalid_0's binary_logloss: 0.154261\n",
      "[64]\tvalid_0's binary_logloss: 0.153742\n",
      "[65]\tvalid_0's binary_logloss: 0.153424\n",
      "[66]\tvalid_0's binary_logloss: 0.152793\n",
      "[67]\tvalid_0's binary_logloss: 0.152668\n",
      "[68]\tvalid_0's binary_logloss: 0.152463\n",
      "[69]\tvalid_0's binary_logloss: 0.152211\n",
      "[70]\tvalid_0's binary_logloss: 0.151806\n",
      "[71]\tvalid_0's binary_logloss: 0.151633\n",
      "[72]\tvalid_0's binary_logloss: 0.151454\n",
      "[73]\tvalid_0's binary_logloss: 0.151329\n",
      "[74]\tvalid_0's binary_logloss: 0.151005\n",
      "[75]\tvalid_0's binary_logloss: 0.150921\n",
      "[76]\tvalid_0's binary_logloss: 0.150725\n",
      "[77]\tvalid_0's binary_logloss: 0.150495\n",
      "[78]\tvalid_0's binary_logloss: 0.15022\n",
      "[79]\tvalid_0's binary_logloss: 0.150066\n",
      "[80]\tvalid_0's binary_logloss: 0.14986\n",
      "[81]\tvalid_0's binary_logloss: 0.149642\n",
      "[82]\tvalid_0's binary_logloss: 0.149398\n",
      "[83]\tvalid_0's binary_logloss: 0.149159\n",
      "[84]\tvalid_0's binary_logloss: 0.149052\n",
      "[85]\tvalid_0's binary_logloss: 0.148866\n",
      "[86]\tvalid_0's binary_logloss: 0.148526\n",
      "[87]\tvalid_0's binary_logloss: 0.148286\n",
      "[88]\tvalid_0's binary_logloss: 0.147905\n",
      "[89]\tvalid_0's binary_logloss: 0.147812\n",
      "[90]\tvalid_0's binary_logloss: 0.14763\n",
      "[91]\tvalid_0's binary_logloss: 0.147621\n",
      "[92]\tvalid_0's binary_logloss: 0.147418\n",
      "[93]\tvalid_0's binary_logloss: 0.147404\n",
      "[94]\tvalid_0's binary_logloss: 0.147172\n",
      "[95]\tvalid_0's binary_logloss: 0.14694\n",
      "[96]\tvalid_0's binary_logloss: 0.146731\n",
      "[97]\tvalid_0's binary_logloss: 0.146464\n",
      "[98]\tvalid_0's binary_logloss: 0.146302\n",
      "[99]\tvalid_0's binary_logloss: 0.146022\n",
      "[100]\tvalid_0's binary_logloss: 0.145955\n",
      "[101]\tvalid_0's binary_logloss: 0.145841\n",
      "[102]\tvalid_0's binary_logloss: 0.145754\n",
      "[103]\tvalid_0's binary_logloss: 0.145725\n",
      "[104]\tvalid_0's binary_logloss: 0.145816\n",
      "[105]\tvalid_0's binary_logloss: 0.145783\n",
      "[106]\tvalid_0's binary_logloss: 0.145532\n",
      "[107]\tvalid_0's binary_logloss: 0.145135\n",
      "[108]\tvalid_0's binary_logloss: 0.14507\n",
      "[109]\tvalid_0's binary_logloss: 0.145078\n",
      "[110]\tvalid_0's binary_logloss: 0.145065\n",
      "[111]\tvalid_0's binary_logloss: 0.145107\n",
      "[112]\tvalid_0's binary_logloss: 0.14509\n",
      "[113]\tvalid_0's binary_logloss: 0.145192\n",
      "[114]\tvalid_0's binary_logloss: 0.145027\n",
      "[115]\tvalid_0's binary_logloss: 0.144958\n",
      "[116]\tvalid_0's binary_logloss: 0.144871\n",
      "[117]\tvalid_0's binary_logloss: 0.144626\n",
      "[118]\tvalid_0's binary_logloss: 0.144684\n",
      "[119]\tvalid_0's binary_logloss: 0.144707\n",
      "[120]\tvalid_0's binary_logloss: 0.14467\n",
      "[121]\tvalid_0's binary_logloss: 0.14474\n",
      "[122]\tvalid_0's binary_logloss: 0.144715\n",
      "[123]\tvalid_0's binary_logloss: 0.144731\n",
      "[124]\tvalid_0's binary_logloss: 0.144916\n",
      "[125]\tvalid_0's binary_logloss: 0.144736\n",
      "[126]\tvalid_0's binary_logloss: 0.144537\n",
      "[127]\tvalid_0's binary_logloss: 0.144482\n",
      "[128]\tvalid_0's binary_logloss: 0.144649\n",
      "[129]\tvalid_0's binary_logloss: 0.14456\n",
      "[130]\tvalid_0's binary_logloss: 0.144636\n",
      "[131]\tvalid_0's binary_logloss: 0.144655\n",
      "[132]\tvalid_0's binary_logloss: 0.144559\n",
      "[133]\tvalid_0's binary_logloss: 0.14442\n",
      "[134]\tvalid_0's binary_logloss: 0.144557\n",
      "[135]\tvalid_0's binary_logloss: 0.144555\n",
      "[136]\tvalid_0's binary_logloss: 0.144451\n",
      "[137]\tvalid_0's binary_logloss: 0.144428\n",
      "[138]\tvalid_0's binary_logloss: 0.144599\n",
      "[139]\tvalid_0's binary_logloss: 0.144621\n",
      "[140]\tvalid_0's binary_logloss: 0.144693\n",
      "[141]\tvalid_0's binary_logloss: 0.144757\n",
      "[142]\tvalid_0's binary_logloss: 0.144683\n",
      "[143]\tvalid_0's binary_logloss: 0.144598\n",
      "[144]\tvalid_0's binary_logloss: 0.144501\n",
      "[145]\tvalid_0's binary_logloss: 0.144543\n",
      "[146]\tvalid_0's binary_logloss: 0.1444\n",
      "[147]\tvalid_0's binary_logloss: 0.14451\n",
      "[148]\tvalid_0's binary_logloss: 0.144504\n",
      "[149]\tvalid_0's binary_logloss: 0.14463\n",
      "[150]\tvalid_0's binary_logloss: 0.144821\n",
      "[151]\tvalid_0's binary_logloss: 0.14491\n",
      "[152]\tvalid_0's binary_logloss: 0.145075\n",
      "[153]\tvalid_0's binary_logloss: 0.145072\n",
      "[154]\tvalid_0's binary_logloss: 0.145179\n",
      "[155]\tvalid_0's binary_logloss: 0.145255\n",
      "[156]\tvalid_0's binary_logloss: 0.145229\n",
      "[157]\tvalid_0's binary_logloss: 0.145214\n",
      "[158]\tvalid_0's binary_logloss: 0.145145\n",
      "[159]\tvalid_0's binary_logloss: 0.145018\n",
      "[160]\tvalid_0's binary_logloss: 0.145004\n",
      "[161]\tvalid_0's binary_logloss: 0.145039\n",
      "[162]\tvalid_0's binary_logloss: 0.144989\n",
      "[163]\tvalid_0's binary_logloss: 0.145092\n",
      "[164]\tvalid_0's binary_logloss: 0.145068\n",
      "[165]\tvalid_0's binary_logloss: 0.145168\n",
      "[166]\tvalid_0's binary_logloss: 0.145289\n",
      "[167]\tvalid_0's binary_logloss: 0.145307\n",
      "[168]\tvalid_0's binary_logloss: 0.145342\n",
      "[169]\tvalid_0's binary_logloss: 0.145332\n",
      "[170]\tvalid_0's binary_logloss: 0.145397\n",
      "[171]\tvalid_0's binary_logloss: 0.14532\n",
      "[172]\tvalid_0's binary_logloss: 0.145321\n",
      "[173]\tvalid_0's binary_logloss: 0.145446\n",
      "[174]\tvalid_0's binary_logloss: 0.145307\n",
      "[175]\tvalid_0's binary_logloss: 0.145216\n",
      "[176]\tvalid_0's binary_logloss: 0.14522\n",
      "[177]\tvalid_0's binary_logloss: 0.145325\n",
      "[178]\tvalid_0's binary_logloss: 0.145422\n",
      "[179]\tvalid_0's binary_logloss: 0.145486\n",
      "[180]\tvalid_0's binary_logloss: 0.145627\n",
      "[181]\tvalid_0's binary_logloss: 0.145761\n",
      "[182]\tvalid_0's binary_logloss: 0.145883\n",
      "[183]\tvalid_0's binary_logloss: 0.145888\n",
      "[184]\tvalid_0's binary_logloss: 0.145924\n",
      "[185]\tvalid_0's binary_logloss: 0.145831\n",
      "[186]\tvalid_0's binary_logloss: 0.145966\n",
      "[187]\tvalid_0's binary_logloss: 0.146043\n",
      "[188]\tvalid_0's binary_logloss: 0.146136\n",
      "[189]\tvalid_0's binary_logloss: 0.146226\n",
      "[190]\tvalid_0's binary_logloss: 0.146328\n",
      "[191]\tvalid_0's binary_logloss: 0.146222\n",
      "[192]\tvalid_0's binary_logloss: 0.146257\n",
      "[193]\tvalid_0's binary_logloss: 0.146244\n",
      "[194]\tvalid_0's binary_logloss: 0.146193\n",
      "[195]\tvalid_0's binary_logloss: 0.146305\n",
      "[196]\tvalid_0's binary_logloss: 0.146376\n",
      "[197]\tvalid_0's binary_logloss: 0.146452\n",
      "[198]\tvalid_0's binary_logloss: 0.146427\n",
      "[199]\tvalid_0's binary_logloss: 0.146544\n",
      "[200]\tvalid_0's binary_logloss: 0.146653\n",
      "[201]\tvalid_0's binary_logloss: 0.146887\n",
      "[202]\tvalid_0's binary_logloss: 0.146941\n",
      "[203]\tvalid_0's binary_logloss: 0.146938\n",
      "[204]\tvalid_0's binary_logloss: 0.147159\n",
      "[205]\tvalid_0's binary_logloss: 0.147167\n",
      "[206]\tvalid_0's binary_logloss: 0.14734\n",
      "[207]\tvalid_0's binary_logloss: 0.147548\n",
      "[208]\tvalid_0's binary_logloss: 0.147636\n",
      "[209]\tvalid_0's binary_logloss: 0.147681\n",
      "[210]\tvalid_0's binary_logloss: 0.147704\n",
      "[211]\tvalid_0's binary_logloss: 0.14776\n",
      "[212]\tvalid_0's binary_logloss: 0.147898\n",
      "[213]\tvalid_0's binary_logloss: 0.147955\n",
      "[214]\tvalid_0's binary_logloss: 0.148051\n",
      "[215]\tvalid_0's binary_logloss: 0.148138\n",
      "[216]\tvalid_0's binary_logloss: 0.148307\n",
      "[217]\tvalid_0's binary_logloss: 0.148332\n",
      "[218]\tvalid_0's binary_logloss: 0.148447\n",
      "[219]\tvalid_0's binary_logloss: 0.148525\n",
      "[220]\tvalid_0's binary_logloss: 0.148652\n",
      "[221]\tvalid_0's binary_logloss: 0.148769\n",
      "[222]\tvalid_0's binary_logloss: 0.148785\n",
      "[223]\tvalid_0's binary_logloss: 0.148953\n",
      "[224]\tvalid_0's binary_logloss: 0.14899\n",
      "[225]\tvalid_0's binary_logloss: 0.149123\n",
      "[226]\tvalid_0's binary_logloss: 0.149188\n",
      "[227]\tvalid_0's binary_logloss: 0.149218\n",
      "[228]\tvalid_0's binary_logloss: 0.149379\n",
      "[229]\tvalid_0's binary_logloss: 0.149617\n",
      "[230]\tvalid_0's binary_logloss: 0.14973\n",
      "[231]\tvalid_0's binary_logloss: 0.149797\n",
      "[232]\tvalid_0's binary_logloss: 0.149821\n",
      "[233]\tvalid_0's binary_logloss: 0.149921\n",
      "[234]\tvalid_0's binary_logloss: 0.150072\n",
      "[235]\tvalid_0's binary_logloss: 0.150143\n",
      "[236]\tvalid_0's binary_logloss: 0.150235\n",
      "[237]\tvalid_0's binary_logloss: 0.150345\n",
      "[238]\tvalid_0's binary_logloss: 0.150386\n",
      "[239]\tvalid_0's binary_logloss: 0.150516\n",
      "[240]\tvalid_0's binary_logloss: 0.150613\n",
      "[241]\tvalid_0's binary_logloss: 0.150671\n",
      "[242]\tvalid_0's binary_logloss: 0.150865\n",
      "[243]\tvalid_0's binary_logloss: 0.150982\n",
      "[244]\tvalid_0's binary_logloss: 0.151072\n",
      "[245]\tvalid_0's binary_logloss: 0.151116\n",
      "[246]\tvalid_0's binary_logloss: 0.151167\n",
      "[247]\tvalid_0's binary_logloss: 0.151327\n",
      "[248]\tvalid_0's binary_logloss: 0.15131\n",
      "[249]\tvalid_0's binary_logloss: 0.151503\n",
      "[250]\tvalid_0's binary_logloss: 0.151615\n",
      "time to train: 469.7s\n",
      "evaluating LGBM:\n",
      "  train f1 score:  0.9926500612494897\n",
      "  test f1 score:  0.9000979431929481\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lg = lgb.LGBMClassifier(learning_rate=0.1,\n",
    "                        n_estimators=400,\n",
    "                        max_bin=255,\n",
    "                        max_depth=40, \n",
    "                        num_iterations=150, \n",
    "                        random_state=11,\n",
    "                        num_leaves=150,\n",
    "                        silent=False)\n",
    "\n",
    "print(\"training LGBM...\")\n",
    "t0 = time.time()\n",
    "lg.fit(X_train, y_train, eval_set=(X_test,y_test), verbose=True)\n",
    "print(f\"time to train: {time.time()-t0:.4}s\")\n",
    "\n",
    "print(\"evaluating LGBM:\")\n",
    "yhat = lg.predict(X_train) \n",
    "print(\"  train f1 score: \", f1_score(y_train, yhat))\n",
    "yhat = lg.predict(X_test) \n",
    "print(\"  test f1 score: \", f1_score(y_test, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
