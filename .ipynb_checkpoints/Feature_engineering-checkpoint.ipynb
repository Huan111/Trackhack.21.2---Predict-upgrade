{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############this module is for feature engineering##############\n",
    "######usage: import Featu\n",
    "\n",
    "import pandas as pd\n",
    "teamname = 'emotional-support-vector-machine-unsw'\n",
    "data_folder='s3://tf-trachack-data/212/'\n",
    "root_folder='s3://tf-trachack-notebooks/'+teamname+'/jupyter/jovyan/'\n",
    "\n",
    "def create_new_data(f_type,name):\n",
    "    '''\n",
    "        Create new data set, should have created new_data folder in your own directory,\n",
    "        f_type: 'dev' or 'eval'\n",
    "    '''\n",
    "    print('Start to creating ....')\n",
    "    data_path = data_folder+\"data/\" + f_type + '/'\n",
    "    to_data_path = name + '/new_data/' + f_type + '_'\n",
    "    \n",
    "    upgrades=pd.read_csv(data_path + \"upgrades.csv\")\n",
    "    \n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    #customer_info\n",
    "    customer_info=pd.read_csv(data_path + \"customer_info.csv\")\n",
    "    customer_info['cus_used_days'] = pd.Series(pd.to_datetime(customer_info['redemption_date']) - pd.to_datetime(customer_info['first_activation_date'])).dt.days\n",
    "    customer_info['cus_used_days'] = scaler.fit_transform(customer_info['cus_used_days'].values.reshape(-1,1))\n",
    "    customer_info['cus_used_days'].fillna(-1,inplace = True)\n",
    "    customer_info['plan_name'].fillna(customer_info['plan_name'].mode()[0], inplace=True)\n",
    "\n",
    "    new_customer_info = pd.get_dummies(customer_info,columns=['carrier', 'plan_name'],drop_first=True)\n",
    "    select_features = ['line_id','cus_used_days', 'carrier_carrier 2', 'carrier_carrier 3','plan_name_plan 1', 'plan_name_plan 2', 'plan_name_plan 3','plan_name_plan 4']\n",
    "    new_customer_info = new_customer_info[select_features].drop_duplicates().reset_index(drop=True)\n",
    "    new_customer_info.to_csv(root_folder+ to_data_path + \"new_customer_info.csv\",header=True,index=None)\n",
    "\n",
    "    #phone_info\n",
    "    phone_info=pd.read_csv(data_path + \"phone_info.csv\")\n",
    "    phone_info['display_description'].fillna('is_miss',inplace=True)\n",
    "    type_list = ['HUAWEI H226C',  'Motorola XT2052DL', 'ZTE Z899VL',\n",
    "            'SAMSUNG S327VL', 'IPHONE 6', 'SAMSUNG G950U',\n",
    "           'IPHONE XR', 'SAMSUNG S260DL', 'LG L52VL', 'LG L322DL',\n",
    "           'SAMSUNG S727VL', 'IPHONE 7 PLUS', 'SAMSUNG S767VL',\n",
    "           'IPHONE 6S PLUS', 'HUAWEI H258C', 'Samsung S102DL',\n",
    "            'LG L555DL', 'IPHONE 7', \n",
    "            'IPHONE 5S', 'LM L413DL', 'ALCATEL A502DL',\n",
    "           'LG L58VL', 'Samsung S215DL', 'SAMSUNG S367VL', 'SAMSUNG S320VL',\n",
    "           'Samsung S111DL', 'KonnectOne K779HSDL', 'Motorola XT2005DL',\n",
    "           'iPhone SE 2', 'LG L455DL', 'SAMSUNG G970U1', 'ALCATEL A405DL',\n",
    "           'SAMSUNG S903VL', 'LG 441G', 'LG LML212VL', 'KonnectOne K500HPEL',\n",
    "           'MOTOROLA XT1925DL', 'LG L355DL', 'HUAWEI H710VL',\n",
    "           'SAMSUNG G930VL', 'LG L423DL', 'MOTOROLA XT1955DL', 'ZTE Z723EL',\n",
    "           'IPHONE SE', 'IPHONE 8', 'IPHONE 8 PLUS', 'SAMSUNG G960U1',\n",
    "           'LM L713DL', 'BLU B110DL', 'LG L164VL', 'ALCATEL A503DL',\n",
    "           'SAMSUNG S120VL', 'SAMSUNG S820L', 'LG LM414DL', 'LG L84VL',\n",
    "           'Samsung S515DL', 'Samsung S115DL', 'Samsung S205DL', 'LG L63BL',\n",
    "           'ZTE Z917VL', 'Motorola XT2041DL', 'LG L82VL', 'ZTE Z291DL',\n",
    "           'ZTE Z799VL', 'SAMSUNG G965U1', 'SAMSUNG S907VL', 'SAMSUNG S357BL',\n",
    "           'LG L158VL', 'LG L125DL', 'IPHONE XS', 'IPHONE 5C', 'ZTE Z233VL',\n",
    "           'ZTE Z963VL', 'LG 235C', 'ALCATEL A577VL', 'LG L44VL',\n",
    "           'SAMSUNG S975L', 'LG L61AL', 'LG L83BL', 'SAMSUNG G955U',\n",
    "           'LG L62VL', 'LG L64VL', 'MOTOROLA XT1920DL', 'LG231C', 'iPhone X',\n",
    "           'SAMSUNG S902L', 'LG 238C', 'SAMSUNG S550TL', 'LG L39C',\n",
    "           'ZTE Z289L', 'SAMSUNG S920L', 'ZTE Z837VL', 'LG 236C', 'LG 440G',\n",
    "           'ZTE Z558VL', 'LG L81AL', 'IPHONE 6 PLUS', 'iPhone 11 Pro Max',\n",
    "           'SAMSUNG S765C', 'iPhone 11', 'SAMSUNG S757BL', 'LG108C',\n",
    "           'SAMSUNG S380C', 'MOTOROLA XT1952DL', 'HUAWEI H883G',\n",
    "           'ALCATEL A501DL', 'IPHONE XS MAX', 'Samsung T528G', 'LG L43AL',\n",
    "           'ALCATEL A450TL', 'ZTE Z795G', 'LG L16C', 'iPhone 12',\n",
    "           'SAMSUNG S906L', 'BRING YOUR TABLET', 'FRANKLIN WIRELESS F900HSVL',\n",
    "           'SAMSUNG S336C', 'iPhone 11 Pro', 'ZTE Z716BL', 'LG LML211BL',\n",
    "           'LG L57BL', 'LG220C', 'LG L21G', 'LG L33L', 'LG L163BL',\n",
    "           'SAMSUNG S730G', 'MOTOROLA INC', 'LG L53BL', 'ZTE Z936L',\n",
    "           'Samsung R451C', 'ZTE Z288L', 'ALCATEL A574BL', 'ALCATEL A621BL',\n",
    "           'ZTE Z557BL', 'ALCATEL A564C', 'ALCATEL A521L', 'LG L35G',\n",
    "           'LG L22C', 'LG L157BL', 'Samsung N981U1', 'BLU B100DL', 'LG 306G',\n",
    "           'SAMSUNG S890L', 'RELIANCE AX54NC', 'SAMSUNG S968C',\n",
    "           'HUAWEI H210C', 'LG 221C', 'HUAWEI H892L', 'LG L51AL', 'ZTE Z796C',\n",
    "           'ZTE Z930L', 'ZTE Z719DL', 'ALCATEL A571VL', 'ALCATEL A392G',\n",
    "           'Alcatel A508DL', 'LG 442BG', 'HUAWEI H715BL', 'ZTE Z986DL',\n",
    "           'LG L31L', 'ZTE Z932L', 'ZTE Z916BL', 'Samsung G981U1',\n",
    "           'NOKIA E5G', 'LG L59BL', 'SAMSUNG G973U1', 'Samsung G770U1',\n",
    "           'SAMSUNG G975U1', 'LG L15G', 'LG L86C', 'LG 237C',\n",
    "           'MOTOROLA W419G', 'iPhone 12 Pro Max', 'Samsung N975U1',\n",
    "           'ZTE Z791G', 'ALCATEL A462C']\n",
    "\n",
    "    phone_info['fm_radio'].fillna('is_miss',inplace = True)\n",
    "    phone_info['available_online'].fillna('is_miss',inplace=True)\n",
    "    phone_info['device_type'].fillna('is_miss',inplace=True)\n",
    "    phone_info['device_type'].replace(['M2M','BYOT','MOBILE_BROADBAND','FEATURE_PHONE','WIRELESS_HOME_PHONE'], ['Others','Others','Others','Others','Others'], inplace=True)\n",
    "    phone_info['display_description'].replace(type_list,['Others']*len(type_list),inplace=True)\n",
    "    phone_info['data_capable'].fillna(0.0,inplace=True)\n",
    "    phone_info['device_lock_state'].fillna('is_miss',inplace=True)\n",
    "    phone_info['device_lock_state'].replace(['LOCKED','UNLOCKED'], ['Others','Others'], inplace=True)\n",
    "    phone_info['bluetooth'].fillna('is_miss',inplace = True)\n",
    "    phone_info['battery_removable'].fillna('is_miss',inplace = True)\n",
    "\n",
    "    new_phone_info = pd.get_dummies(phone_info,columns=['available_online','device_type','device_lock_state','data_capable','bluetooth','battery_removable','fm_radio','display_description'],drop_first=True)\n",
    "    select_features = ['line_id', 'available_online_Y', 'available_online_is_miss',\n",
    "           'device_type_Others', 'device_type_SMARTPHONE', 'device_type_is_miss',\n",
    "           'device_lock_state_UNLOCKABLE', 'device_lock_state_is_miss',\n",
    "           'data_capable_1.0', 'bluetooth_Y', 'bluetooth_is_miss',\n",
    "           'battery_removable_Y', 'battery_removable_is_miss', 'fm_radio_Y',\n",
    "           'fm_radio_is_miss', 'display_description_IPHONE 6S',\n",
    "           'display_description_LG L722DL', 'display_description_Others',\n",
    "           'display_description_Samsung S506DL', 'display_description_is_miss']\n",
    "    new_phone_info = new_phone_info[select_features].drop_duplicates().reset_index(drop=True)\n",
    "    new_phone_info.to_csv(root_folder + to_data_path + \"new_phone_info.csv\",header=True,index=None)\n",
    "\n",
    "    #redemptions\n",
    "    \n",
    "    redemptions=pd.read_csv(data_path + \"redemptions.csv\")\n",
    "\n",
    "    redemptions['red_count'] = scaler.fit_transform(redemptions.groupby('line_id')['channel'].transform('count').values.reshape(-1,1))#how ofen use\n",
    "    redemptions['red_mean_rev'] = scaler.fit_transform(redemptions.groupby('line_id')['gross_revenue'].transform('mean').values.reshape(-1,1))#how much\n",
    "    redemptions['channel_unique'] = scaler.fit_transform(redemptions.groupby('line_id')['channel'].transform('nunique').values.reshape(-1,1))#what kinds of channel\n",
    "    redemptions['red_type_unique'] = scaler.fit_transform(redemptions.groupby('line_id')['redemption_type'].transform('nunique').values.reshape(-1,1))# what kinds of paid type\n",
    "    redemptions['red_type_most_fre'] = redemptions['line_id'].map(redemptions.groupby('line_id')['redemption_type'].agg(lambda x: x.value_counts().idxmax()))#most frequently paid type\n",
    "    redemptions['channel_most_fre'] = redemptions['line_id'].map(redemptions.groupby('line_id')['channel'].agg(lambda x: x.value_counts().idxmax()))#most frequently channel\n",
    "    redemptions=pd.get_dummies(redemptions,columns=['red_type_most_fre','channel_most_fre'],drop_first=True)\n",
    "    redemptions = pd.merge(upgrades,redemptions,how='left',on='line_id')\n",
    "    lst = ['red_count','red_mean_rev','channel_unique','red_type_unique']\n",
    "    redemptions[lst] = redemptions[lst].fillna(redemptions[lst].mean())\n",
    "    redemptions.fillna(0,inplace=True)\n",
    "    \n",
    "    select_features = [e for e in redemptions.columns if e not in ['channel','gross_revenue','redemption_date','redemption_type','revenue_type']]\n",
    "    new_redemptions = redemptions[select_features].drop_duplicates().reset_index(drop=True)\n",
    "    new_redemptions.to_csv(root_folder+ to_data_path + \"new_redemptions.csv\",header=True,index=None)\n",
    "\n",
    "    #deactivations\n",
    "    deactivations=pd.read_csv(data_path + \"deactivations.csv\")\n",
    "\n",
    "    deactivations['dea_times'] = scaler.fit_transform(deactivations.groupby('line_id')['deactivation_date'].transform('count').values.reshape(-1,1))\n",
    "    deactivations['dea_reason_uni_counts'] = scaler.fit_transform(deactivations.groupby('line_id')['deactivation_reason'].transform('nunique').values.reshape(-1,1))\n",
    "    deactivations['dea_most_fre_reason'] = deactivations['line_id'].map(deactivations.groupby('line_id')['deactivation_reason'].agg(lambda x: x.value_counts().idxmax()))\n",
    "\n",
    "    temp_list = ['STOLEN','REMOVED_FROM_GROUP','MINCHANGE','STOLEN CREDIT CARD','DEVICE CHANGE INQUIRY','PORTED NO A/I','WN-SYSTEM ISSUED']\n",
    "    deactivations['dea_most_fre_reason'].replace(temp_list,['Other']*len(temp_list),inplace=True)\n",
    "    deactivations=pd.get_dummies(deactivations,columns=['dea_most_fre_reason'])#not useing drop first for merging the upgrade line_id and fillna with 0\n",
    "\n",
    "    new_deactivation = pd.merge(upgrades,deactivations,how='left',on = 'line_id')\n",
    "    new_deactivation['dea_times'].fillna(-1,inplace=True)#have no deactivation record\n",
    "    new_deactivation.fillna(0,inplace=True)#have no deactivation record\n",
    "\n",
    "    select_features = ['line_id', 'dea_times',\n",
    "           'dea_reason_uni_counts', 'dea_most_fre_reason_Other',\n",
    "           'dea_most_fre_reason_PASTDUE', 'dea_most_fre_reason_PORT OUT',\n",
    "           'dea_most_fre_reason_RISK ASSESSMENT', 'dea_most_fre_reason_UPGRADE','dea_most_fre_reason_CUSTOMER REQD']\n",
    "    new_deactivation = new_deactivation[select_features].drop_duplicates().reset_index(drop=True)\n",
    "    new_deactivation.to_csv(root_folder+ to_data_path+ \"new_deactivation.csv\",header=True,index=None)\n",
    "\n",
    "    #suspensions\n",
    "    suspensions=pd.read_csv(data_path + \"suspensions.csv\")\n",
    "    suspensions['sus_count'] = scaler.fit_transform(suspensions.groupby('line_id')['suspension_start_date'].transform('count').values.reshape(-1,1))\n",
    "    new_suspension = pd.merge(upgrades,suspensions,how='left',on = 'line_id')\n",
    "    new_suspension['sus_count'].fillna(-1,inplace=True)\n",
    "    select_features = ['line_id','sus_count']\n",
    "    new_suspension = new_suspension[select_features].drop_duplicates().reset_index(drop=True)\n",
    "    new_suspension.to_csv(root_folder+ to_data_path +\"new_suspension.csv\",header=True,index=None)\n",
    "\n",
    "    #network_usage\n",
    "    network_usage_domestic=pd.read_csv(data_path + \"network_usage_domestic.csv\")\n",
    "\n",
    "    network_usage_domestic['network_used_day'] = network_usage_domestic.groupby('line_id')['date'].transform('count')\n",
    "\n",
    "    features = ['line_id', 'hotspot_kb', 'mms_in', 'mms_out', 'sms_in',\n",
    "           'sms_out', 'total_kb', 'voice_count_in', 'voice_count_total',\n",
    "           'voice_min_in', 'voice_min_out']\n",
    "    new_features = ['mean_hotspot_kb', 'mean_mms_in', 'mean_mms_out', 'mean_sms_in',\n",
    "           'mean_sms_out', 'mean_total_kb', 'mean_voice_count_in', 'mean_voice_count_total',\n",
    "           'mean_voice_min_in', 'mean_voice_min_out']\n",
    "\n",
    "    temp = network_usage_domestic[features].groupby('line_id').transform('mean')\n",
    "    temp.columns = new_features\n",
    "\n",
    "    new_network_usage_domestic = pd.concat((network_usage_domestic,temp),axis=1)\n",
    "\n",
    "    new_features = ['network_used_day','mean_hotspot_kb', 'mean_mms_in', 'mean_mms_out', 'mean_sms_in',\n",
    "           'mean_sms_out', 'mean_total_kb', 'mean_voice_count_in', 'mean_voice_count_total',\n",
    "           'mean_voice_min_in', 'mean_voice_min_out']\n",
    "\n",
    "    new_network_usage_domestic[new_features] = scaler.fit_transform(new_network_usage_domestic[new_features])\n",
    "\n",
    "    #merge upgrade id_line and fill with mean\n",
    "    new_network_usage_domestic = pd.merge(upgrades,new_network_usage_domestic,how='left',on='line_id')\n",
    "    features = ['network_used_day','mean_hotspot_kb', 'mean_mms_in', 'mean_mms_out', 'mean_sms_in',\n",
    "           'mean_sms_out', 'mean_total_kb', 'mean_voice_count_in', 'mean_voice_count_total',\n",
    "           'mean_voice_min_in', 'mean_voice_min_out']\n",
    "    new_network_usage_domestic[features] = new_network_usage_domestic[features].fillna((new_network_usage_domestic[features].mean()))\n",
    "\n",
    "    #populate table\n",
    "    select_features = ['line_id','network_used_day','mean_hotspot_kb', 'mean_mms_in', 'mean_mms_out', 'mean_sms_in',\n",
    "           'mean_sms_out', 'mean_total_kb', 'mean_voice_count_in', 'mean_voice_count_total',\n",
    "           'mean_voice_min_in', 'mean_voice_min_out']\n",
    "    new_network_usage_domestic = new_network_usage_domestic[select_features].drop_duplicates().reset_index(drop=True)\n",
    "    new_network_usage_domestic.to_csv(root_folder+ to_data_path + \"new_networ_usage_domestic.csv\",header=True,index=None)\n",
    "    print('Finished.')\n",
    "    \n",
    "#merge all tables\n",
    "def merge_tables(f_type,name):\n",
    "    \"\"\"\n",
    "    merge the tables, f_type: 'dev' or 'eval'\n",
    "    must create new_data folder under your working folder\n",
    "    \"\"\"\n",
    "    print('Start to merge...')\n",
    "    data_path = name + '/new_data/' + f_type + '_'\n",
    "\n",
    "    new_redemptions = pd.read_csv(root_folder+ data_path + \"new_redemptions.csv\")\n",
    "    new_phone_info = pd.read_csv(root_folder+ data_path + \"new_phone_info.csv\")\n",
    "    new_customer_info = pd.read_csv(root_folder+ data_path +\"new_customer_info.csv\")\n",
    "    new_deactivation = pd.read_csv(root_folder+data_path + \"new_deactivation.csv\")\n",
    "    new_suspension = pd.read_csv(root_folder+ data_path +\"new_suspension.csv\")\n",
    "    new_network_usage_domestic = pd.read_csv(root_folder+ data_path +\"new_networ_usage_domestic.csv\")\n",
    "    upgrades=pd.read_csv(data_folder+\"data/\" + f_type + \"/upgrades.csv\")\n",
    "    upgrades = upgrades[['line_id']]\n",
    "\n",
    "    table_list = [new_redemptions,new_phone_info,new_customer_info,new_deactivation,new_suspension,new_network_usage_domestic,upgrades]\n",
    "    final_merge = pd.concat(table_list, join='inner', axis=1)\n",
    "    final_merge = final_merge.loc[:,~final_merge.columns.duplicated()]\n",
    "    final_merge.to_csv(root_folder + data_path + \"final_merge.csv\",header=True,index=None)\n",
    "    print('Finished')\n",
    "\n",
    "#call above functions\n",
    "def main(f_type,name):\n",
    "    create_new_data(f_type,name)\n",
    "    merge_tables(f_type,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "teamname = 'emotional-support-vector-machine-unsw'\n",
    "root_folder='s3://tf-trachack-notebooks/'+teamname+'/jupyter/jovyan/'\n",
    "\n",
    "data_train = pd.read_csv(root_folder+\"guohuan-li/new_data/dev_final_merge.csv\")\n",
    "data_val = pd.read_csv(root_folder+\"guohuan-li/new_data/eval_final_merge.csv\")\n",
    "\n",
    "#drop some features not in both datasets\n",
    "train_lst = list(data_train.columns[3:])\n",
    "val_lst = list(data_val.columns[1:])\n",
    "drop_lst = np.setdiff1d(train_lst,val_lst)\n",
    "data_train.drop(drop_lst, axis=1,inplace=True)\n",
    "train_lst = list(data_train.columns[3:])\n",
    "val_lst = list(data_val.columns[1:])\n",
    "drop_lst = np.setdiff1d(val_lst,train_lst)\n",
    "data_val.drop(drop_lst, axis=1,inplace=True)\n",
    "\n",
    "#extract the training data\n",
    "data_y = data_train['upgrade'].replace({'no':0,'yes':1})\n",
    "data_X = data_train.drop(['line_id','upgrade_date','upgrade'],axis = 1)\n",
    "data_val_X = data_val.drop(['line_id'],axis = 1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "DTC = DecisionTreeClassifier()\n",
    "RFC = RandomForestClassifier()\n",
    "ABC = AdaBoostClassifier()\n",
    "LR = LogisticRegression(max_iter=500)\n",
    "MLP = MLPClassifier(max_iter = 500)\n",
    "SVM = SVC()\n",
    "\n",
    "clfs = [DTC,RFC,ABC,LR,MLP,SVM]\n",
    "names = ['DTC','RFC','ABC','LR','MLP','SVM']\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "scoring = ['f1','precision','recall','accuracy']\n",
    "\n",
    "for i in range(len(names)):\n",
    "    scores  = cross_validate(clfs[i],data_X,data_y,cv = 10,scoring = scoring,return_train_score=True)\n",
    "    print(f'The model {names[i]} f1 is {scores[\"test_f1\"].mean()}, accu is {scores[\"test_accuracy\"].mean()}')\n",
    "    print()\n",
    "\n",
    "#RFC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'bootstrap': [True, False],\n",
    " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    " 'max_features': ['auto', 'sqrt'],\n",
    " 'min_samples_leaf': [1, 2, 4],\n",
    " 'min_samples_split': [2, 5, 10],\n",
    " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000],\n",
    " 'criterion' :['gini', 'entropy']}\n",
    "CV_rfc = GridSearchCV(estimator=RFC, param_grid=param_grid, cv= 10,n_jobs=-1,scoring = 'f1')\n",
    "CV_rfc.fit(data_X,data_y)\n",
    "CV_rfc.best_params_\n",
    "\n",
    "#Adaboost\n",
    "param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n",
    "              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n",
    "              \"n_estimators\": [10, 50, 100, 500],\n",
    "              'learning_rate' : [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "             }\n",
    "CV_abc = GridSearchCV(estimator=ABC, param_grid=param_grid, cv= 10,n_jobs=-1,scoring = 'f1')\n",
    "CV_abc.fit(data_X,data_y)\n",
    "CV_abc.best_params_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####ver2\n",
    "####The order of feature engineering\n",
    "####1. Data explore: Barplot,boxplot, see any outlier and categorical features number\n",
    "####2. Missing value fill: numerical fill with mean or mode, categorical fill with 'ismiss' or mode\n",
    "####\n",
    "import matplotlib.pyplot as plt\n",
    "def discrete_var_barplot(x,y,data):\n",
    "    plt.figure(figsize=(15,10))\n",
    "    sns.barplot(x=x,y=y,data=data)\n",
    "\n",
    "def discrete_var_boxplot(x,y,data):\n",
    "    plt.figure(figsize=(15,10))\n",
    "    sns.boxplot(x=x,y=y,data=data)\n",
    "    \n",
    "def merge_table(left,right,on,how):\n",
    "    return pd.merge(left,right,on = on,how = how)\n",
    "\n",
    "def discrete_var_countplot(x,data):\n",
    "    plt.figure(figsize=(15,10))\n",
    "    sns.countplot(x=x,data=data)\n",
    "   \n",
    "#The correlation map\n",
    "def correlation_plot(data):\n",
    "    corrmat = data.corr()\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(15,15)\n",
    "    sns.heatmap(corrmat,cmap=\"YlGnBu\",linewidths=.5,annot=True)\n",
    "    \n",
    "#convert categorical features to number\n",
    "def cat_to_num(x,data):\n",
    "    return data[x].astype('category').cat.codes\n",
    "\n",
    "#fill na with most frequent value\n",
    "def fill_na_with_fre(x,data):\n",
    "    return data[x].fillna(data[x].mode()[0])\n",
    "\n",
    "#fill na with perticular value\n",
    "def fill_na_with_val(num,na_col,data):\n",
    "    return data[na_col].fillna(num)\n",
    "\n",
    "#detect the outlier by IQR\n",
    "def detect_outlier_IQR(data,col,threshold):\n",
    "    IQR = data[col].quantile(0.75) - data[col].quantile(0.25)\n",
    "    Lower_fence = data[col].quantile(0.25) - (IQR * threshold)\n",
    "    Upper_fence = data[col].quantile(0.75) + (IQR * threshold)\n",
    "    tmp = pd.concat([data[col]>Upper_fence,data[col]<Lower_fence],axis=1)\n",
    "    outlier_index = tmp.any(axis=1)\n",
    "    try:\n",
    "        outlier_num = outlier_index.value_counts()[1]\n",
    "    except:\n",
    "        outlier_num = 0\n",
    "    return outlier_index, outlier_num\n",
    "\n",
    "#replace outlier by mean or most frequent values\n",
    "def replace_outlier(data,col,idx,method):\n",
    "    data_copy = data.copy(deep=True)\n",
    "    if method == 'mean':\n",
    "        data_copy.loc[idx,col] = data_copy[col].mean()\n",
    "    else:\n",
    "        data_copy.loc[idx,col] = data_copy[col].mode()[0]\n",
    "    return data_copy\n",
    "\n",
    "#relace rare value with different value\n",
    "def replace_rare_value(data,col,threshold):\n",
    "    temp_df = pd.Series(data[col].value_counts()/len(data))\n",
    "    mapping = { k: ('other' if k not in temp_df[temp_df >= threshold].index else k)\n",
    "                for k in temp_df.index}\n",
    "    return data[col].replace(mapping)\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "#chi test for feature selections\n",
    "def chi_test(data,X,y):\n",
    "    y_val = data[y]\n",
    "    chi_scores = chi2(data[X],y_val)\n",
    "    p_values = pd.Series(chi_scores[1],index = data[X].columns)\n",
    "    p_values.sort_values(ascending = False , inplace = True)\n",
    "    p_values.plot.bar()\n",
    "    return p_values\n",
    "\n",
    "#select the features based on chi test p-values\n",
    "def select_fea_by_chi(data,p_vals,threshold):\n",
    "    drop_cols = p_vals[p_vals >= threshold].index\n",
    "    return data.drop(drop_cols,axis=1)\n",
    "\n",
    "import scipy.stats as stats\n",
    "#student-t test for numerical features\n",
    "def t_test(temp,X,y):\n",
    "    population = temp[temp[y] == 0][X].mean()\n",
    "    return stats.ttest_1samp(a = temp[temp[y]==1][X],popmean = population)\n",
    "\n",
    "#select features by t test\n",
    "def select_fea_by_t(data,test,threshold):\n",
    "    columns = test.statistic.index\n",
    "    drop_cols = columns[test.pvalue >= threshold]\n",
    "    return data.drop(drop_cols,axis=1)\n",
    "    \n",
    "import numpy as np\n",
    "#remove features based on correlation\n",
    "def remove_features_cor(data,corr_score=0.9):\n",
    "    corr = data.corr()\n",
    "    columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "    for i in range(corr.shape[0]):\n",
    "        for j in range(i+1, corr.shape[0]):\n",
    "            if corr.iloc[i,j] >= corr_score:\n",
    "                if columns[j]:\n",
    "                    columns[j] = False\n",
    "    select_columns = data.columns[columns]\n",
    "    return data[select_columns]\n",
    "#create feature by groupby transform\n",
    "def groupby_transform(data,col,by,method):\n",
    "    return data.groupby(by)[col].transform(method)\n",
    "\n",
    "#create feature by groupby agg\n",
    "def groupby_agg(data,col,by,func):\n",
    "    return data[by].map(data.groupby(by)[col].agg(func))\n",
    "\n",
    "def create_new_data(f_type,name):\n",
    "    '''\n",
    "        Create new data set, should have created new_data folder in your own directory,\n",
    "        f_type: 'dev' or 'eval'\n",
    "    '''\n",
    "    data_path = data_folder+\"data/\" + f_type + '/'\n",
    "    to_data_path = name + '/new_data/' + f_type + '_'\n",
    "    print('Starting to creating...')\n",
    "    upgrades=pd.read_csv(data_path + \"upgrades.csv\")\n",
    "    \n",
    "    customer_info=pd.read_csv(data_path + \"customer_info.csv\")\n",
    "    customer_info['plan_name'] = fill_na_with_fre('plan_name',customer_info)\n",
    "    customer_info['plan_name'] = cat_to_num('plan_name',customer_info)\n",
    "    customer_info['carrier'] = cat_to_num('carrier',customer_info)\n",
    "\n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    customer_info['cus_used_days'] = pd.Series(pd.to_datetime(customer_info['redemption_date']) - pd.to_datetime(customer_info['first_activation_date'])).dt.days\n",
    "    outlier_idx,outlier_num = detect_outlier_IQR(customer_info,'cus_used_days',3)\n",
    "    if outlier_num != 0:\n",
    "        customer_info = replace_outlier(customer_info,'cus_used_days',outlier_idx,'mean')\n",
    "    scaler = RobustScaler()\n",
    "    customer_info['cus_used_days'] = fill_na_with_val(-999,'cus_used_days',customer_info)\n",
    "    customer_info['cus_used_days'] = scaler.fit_transform(customer_info['cus_used_days'].values.reshape(-1,1))\n",
    "    select_features = ['line_id','cus_used_days', 'plan_name','carrier']\n",
    "    new_customer_info = customer_info[select_features].drop_duplicates().reset_index(drop=True)\n",
    "    new_customer_info.to_csv(root_folder+ to_data_path + \"new_customer_info.csv\",header=True,index=None)\n",
    "\n",
    "    #from sklearn.preprocessing import LabelEncoder\n",
    "    phone_info=pd.read_csv(data_path + \"phone_info.csv\")\n",
    "    phone_upg = merge_table(upgrades,phone_info,on='line_id',how='left')\n",
    "    temp_features = ['display_description','battery_removable','bluetooth','available_online','device_lock_state','device_type','fm_radio','has_wifi_calling','manufacturer','mobile_hotspot','model','model_type','mp3player','multi_call','operating_system','phone_gen','technology','unlock_elegible','unlock_type']\n",
    "    #label_encoder = LabelEncoder()\n",
    "    for i in temp_features:\n",
    "        phone_upg[i] = fill_na_with_val('ismiss',i,phone_upg)\n",
    "        phone_upg[i] = replace_rare_value(phone_upg,i,0.02)\n",
    "        #phone_upg[i] = label_encoder.fit_transform(phone_upg[i])\n",
    "\n",
    "    #chi_test(phone_upg,temp_features,'upgrade')\n",
    "    #correlation_plot(phone_upg[temp_features])\n",
    "    #new_phone = remove_features_cor(phone_upg[temp_features],0.88)\n",
    "    #new_phone = pd.concat((phone_upg['line_id'],remove_features_cor(phone_upg[temp_features],0.88)),axis=1)\n",
    "    select_features = ['line_id', 'display_description', 'battery_removable', 'bluetooth',\n",
    "       'device_type', 'fm_radio', 'has_wifi_calling', 'manufacturer',\n",
    "       'mobile_hotspot', 'model', 'model_type', 'operating_system',\n",
    "       'phone_gen', 'technology']\n",
    "    \n",
    "    phone_upg[select_features].to_csv(root_folder+ to_data_path + \"new_phone_info.csv\",header=True,index=None)\n",
    "\n",
    "    redemptions=pd.read_csv(data_path + \"redemptions.csv\")\n",
    "    redemptions['red_count'] = groupby_transform(redemptions,'channel','line_id','count')\n",
    "    redemptions['red_mean_rev'] = groupby_transform(redemptions,'gross_revenue','line_id','mean')\n",
    "    redemptions['channel_unique'] = groupby_transform(redemptions,'channel','line_id','nunique')\n",
    "    redemptions['red_type_unique'] = groupby_transform(redemptions,'redemption_type','line_id','nunique')\n",
    "    redemptions['rev_type_unique'] = groupby_transform(redemptions,'revenue_type','line_id','nunique')\n",
    "    redemptions['channel_most_fre'] = groupby_agg(redemptions,'channel','line_id',lambda x: x.value_counts().idxmax())\n",
    "    redemptions['red_type_most_fre'] = groupby_agg(redemptions,'redemption_type','line_id',lambda x: x.value_counts().idxmax())\n",
    "    redemptions['rev_type_most_fre'] = groupby_agg(redemptions,'revenue_type','line_id',lambda x: x.value_counts().idxmax())\n",
    "    use_feature = ['line_id','red_count','red_mean_rev','channel_unique','red_type_unique','rev_type_unique','channel_most_fre','red_type_most_fre','rev_type_most_fre']\n",
    "    new_redemptions = redemptions[use_feature].drop_duplicates().reset_index(drop=True)\n",
    "    new_redemptions = merge_table(upgrades,new_redemptions,'line_id','left')\n",
    "\n",
    "    for i in use_feature[1:]:\n",
    "        new_redemptions[i] = fill_na_with_fre(i,new_redemptions)\n",
    "\n",
    "    for i in ['red_count','red_mean_rev']:\n",
    "        outlier_idx,outlier_num = detect_outlier_IQR(new_redemptions,i,3)\n",
    "        if outlier_num != 0:\n",
    "            new_redemptions = replace_outlier(new_redemptions,i,outlier_idx,'mean')\n",
    "\n",
    "    new_redemptions['channel_most_fre'] = replace_rare_value(new_redemptions,'channel_most_fre',0.02)\n",
    "    new_redemptions['red_type_most_fre'] = replace_rare_value(new_redemptions,'red_type_most_fre',0.02)\n",
    "    new_redemptions['rev_type_most_fre'] = replace_rare_value(new_redemptions,'rev_type_most_fre',0.02)\n",
    "    cat_features = ['channel_most_fre','red_type_most_fre','rev_type_most_fre']\n",
    "    #label_encoder = LabelEncoder()\n",
    "    #for i in cat_features:\n",
    "    #    new_redemptions[i] = label_encoder.fit_transform(new_redemptions[i])\n",
    "    \n",
    "    #p_vals = chi_test(new_redemptions,['channel_most_fre','red_type_most_fre','rev_type_most_fre','channel_unique','red_type_unique','rev_type_unique'],'upgrade')\n",
    "    #new_redemptions = select_fea_by_chi(new_redemptions,p_vals,0.05)\n",
    "    #temp = t_test(new_redemptions,['red_count','red_mean_rev'],'upgrade')\n",
    "    #new_redemptions = select_fea_by_t(new_redemptions,temp,0.05)\n",
    "    scaler = RobustScaler()\n",
    "    for i in ['red_count','red_mean_rev']:\n",
    "        new_redemptions[i] = scaler.fit_transform(new_redemptions[i].values.reshape(-1,1))\n",
    "    select_features = ['line_id','red_count','red_mean_rev','channel_unique','channel_most_fre']\n",
    "    new_redemptions[select_features].to_csv(root_folder+ to_data_path + \"new_redemptions.csv\",header=True,index=None)\n",
    "\n",
    "    deactivations=pd.read_csv(data_path + \"deactivations.csv\")\n",
    "    reactivations=pd.read_csv(data_path + \"reactivations.csv\")\n",
    "    dea_rea_info = merge_table(deactivations,reactivations,on='line_id',how='inner')\n",
    "    dea_rea_upg = merge_table(upgrades,dea_rea_info,'line_id','left')\n",
    "    dea_rea_upg['deactivation_reason'] = fill_na_with_fre('deactivation_reason',dea_rea_upg)\n",
    "    dea_rea_upg['reactivation_channel'] = fill_na_with_fre('reactivation_channel',dea_rea_upg)\n",
    "    dea_rea_upg['de_re_counts'] = groupby_transform(dea_rea_upg,'deactivation_date','line_id','count')\n",
    "    dea_rea_upg['reason_unique'] = groupby_transform(dea_rea_upg,'deactivation_reason','line_id','nunique')\n",
    "    dea_rea_upg['de_re_channel_unique'] = groupby_transform(dea_rea_upg,'reactivation_channel','line_id','nunique')\n",
    "    dea_rea_upg['de_re_channel_most_fre'] = groupby_agg(dea_rea_upg,'reactivation_channel','line_id',lambda x: x.value_counts().idxmax())\n",
    "    dea_rea_upg['de_re_reason_most_fre'] = groupby_agg(dea_rea_upg,'deactivation_reason','line_id',lambda x: x.value_counts().idxmax())\n",
    "    use_features = ['line_id','de_re_counts','reason_unique','de_re_channel_unique','de_re_channel_most_fre','de_re_reason_most_fre']\n",
    "    new_dea_rea = dea_rea_upg[use_features].drop_duplicates().reset_index(drop=True)\n",
    "    for i in ['de_re_channel_most_fre','de_re_reason_most_fre']:\n",
    "        new_dea_rea[i] = replace_rare_value(new_dea_rea,i,0.02)\n",
    "    outlier_index,outlier_num = detect_outlier_IQR(new_dea_rea,'de_re_counts',3)\n",
    "    #new_dea_rea[outlier_index]\n",
    "    if outlier_num!= 0:\n",
    "        new_dea_rea = replace_outlier(new_dea_rea,'de_re_counts',outlier_index,'mean')\n",
    "    cat_features = ['de_re_channel_most_fre','de_re_reason_most_fre']\n",
    "    #for i in cat_features:\n",
    "    #    new_dea_rea[i] = label_encoder.fit_transform(new_dea_rea[i])\n",
    "    \n",
    "    #new_dea_rea['upgrade'] = dea_rea_upg['upgrade']\n",
    "    #p_vals = chi_test(new_dea_rea,cat_features,'upgrade')\n",
    "    #new_dea_rea = select_fea_by_chi(new_dea_rea,p_vals,0.05)\n",
    "    #use_features = ['de_re_counts','reason_unique','channel_unique']\n",
    "    #temp = t_test(new_dea_rea,use_features,'upgrade')\n",
    "    #new_dea_rea = select_fea_by_t(new_dea_rea,temp,0.05)\n",
    "    select_features = ['line_id','de_re_channel_unique','de_re_channel_most_fre']\n",
    "    new_dea_rea[select_features].to_csv(root_folder+ to_data_path + \"new_rea_dea.csv\",header=True,index=None)\n",
    "\n",
    "    suspensions=pd.read_csv(data_path + \"suspensions.csv\")\n",
    "    suspensions['sus_count'] = groupby_transform(suspensions,'suspension_start_date','line_id','count')\n",
    "    suspensions = suspensions[['line_id','sus_count']].drop_duplicates().reset_index(drop=True)\n",
    "    new_suspensions = merge_table(upgrades,suspensions,'line_id','left')\n",
    "    new_suspensions['sus_count'] = fill_na_with_fre('sus_count',new_suspensions)\n",
    "    outlier_index,outlier_num = detect_outlier_IQR(new_suspensions,'sus_count',3)\n",
    "    if outlier_num!=0:\n",
    "        new_suspensions = replace_outlier(new_suspensions,'sus_count',outlier_index,'mean')\n",
    "    scaler = RobustScaler()\n",
    "    new_suspensions['sus_count'] = scaler.fit_transform(new_suspensions['sus_count'].values.reshape(-1,1))\n",
    "    new_suspensions[['line_id','sus_count']].to_csv(root_folder+ to_data_path + \"new_suspensions.csv\",header=True,index=None)\n",
    "\n",
    "    network_usage_domestic=pd.read_csv(data_path + \"network_usage_domestic.csv\")\n",
    "    network_usage_domestic['net_work_mean_kb'] = groupby_transform(network_usage_domestic,'total_kb','line_id','mean')\n",
    "    network_usage_domestic['net_work_count'] = groupby_transform(network_usage_domestic,'date','line_id','count')\n",
    "    network_usage_domestic = network_usage_domestic[['line_id','net_work_mean_kb','net_work_count']]\n",
    "    network_usage_domestic = network_usage_domestic.drop_duplicates().reset_index(drop=True)\n",
    "    new_network_usage_domestic = merge_table(upgrades,network_usage_domestic,'line_id','left')\n",
    "    scaler = RobustScaler()\n",
    "    for i in ['net_work_mean_kb','net_work_count']:\n",
    "        new_network_usage_domestic[i] = fill_na_with_val(new_network_usage_domestic[i].mean(),i,new_network_usage_domestic)\n",
    "        outlier_index,outlier_num = detect_outlier_IQR(new_network_usage_domestic,i,3)\n",
    "        if outlier_num !=0:\n",
    "            new_network_usage_domestic = replace_outlier(new_network_usage_domestic,i,outlier_index,'mean')\n",
    "        new_network_usage_domestic[i] = scaler.fit_transform(new_network_usage_domestic[i].values.reshape(-1,1))\n",
    "    new_network_usage_domestic[['line_id','net_work_mean_kb','net_work_count']].to_csv(root_folder+ to_data_path + \"new_network_usage_domestic.csv\",header=True,index=None)\n",
    "    print('Finished')\n",
    "    \n",
    "def merge_tables(f_type,name):\n",
    "    \"\"\"\n",
    "    merge the tables, f_type: 'dev' or 'eval'\n",
    "    must create new_data folder under your working folder\n",
    "    \"\"\"\n",
    "    print('Start to merge...')\n",
    "    data_path = name + '/new_data/' + f_type + '_'\n",
    "\n",
    "    new_redemptions = pd.read_csv(root_folder+ data_path + \"new_redemptions.csv\")\n",
    "    new_phone_info = pd.read_csv(root_folder+ data_path + \"new_phone_info.csv\")\n",
    "    new_customer_info = pd.read_csv(root_folder+ data_path +\"new_customer_info.csv\")\n",
    "    new_deactivation = pd.read_csv(root_folder+data_path + \"new_rea_dea.csv\")\n",
    "    new_suspension = pd.read_csv(root_folder+ data_path +\"new_suspensions.csv\")\n",
    "    new_network_usage_domestic = pd.read_csv(root_folder+ data_path +\"new_network_usage_domestic.csv\")\n",
    "    upgrades=pd.read_csv(data_folder+\"data/\" + f_type + \"/upgrades.csv\")\n",
    "\n",
    "    table_list = [new_redemptions,new_phone_info,new_customer_info,new_deactivation,new_suspension,new_network_usage_domestic,upgrades]\n",
    "    final_merge = pd.concat(table_list, join='inner', axis=1)\n",
    "    final_merge = final_merge.loc[:,~final_merge.columns.duplicated()]\n",
    "    final_merge.to_csv(root_folder + data_path + \"final_merge.csv\",header=True,index=None)\n",
    "    print('Finished')\n",
    "\n",
    "create_new_data('dev','guohuan-li')\n",
    "create_new_data('eval','guohuan-li')\n",
    "merge_tables('dev','guohuan-li')\n",
    "merge_tables('eval','guohuan-li')\n",
    "\n",
    "data_path = 'guohuan-li' + '/new_data/' + 'dev' + '_'\n",
    "merge_train = pd.read_csv(root_folder+ data_path + \"final_merge.csv\")\n",
    "\n",
    "data_path = 'guohuan-li' + '/new_data/' + 'eval' + '_'\n",
    "merge_val = pd.read_csv(root_folder+ data_path + \"final_merge.csv\")\n",
    "\n",
    "merge_val['channel_unique'].replace(7,6,inplace=True)\n",
    "merge_val['display_description'].replace('SAMSUNG S727VL','other',inplace=True)\n",
    "merge_val['model'].replace('STSAS727VL','other',inplace=True)\n",
    "merge_val['model_type'].replace('Flip','other',inplace=True)\n",
    "merge_train['operating_system'].replace('other','PROPRIETARY',inplace=True)\n",
    "merge_train['de_re_channel_unique'].replace(6,5,inplace=True)\n",
    "\n",
    "cat_features = ['channel_most_fre', 'display_description', 'battery_removable',\n",
    "       'bluetooth', 'device_type', 'fm_radio', 'has_wifi_calling',\n",
    "       'manufacturer', 'mobile_hotspot', 'model', 'model_type',\n",
    "       'operating_system', 'phone_gen', 'technology','de_re_channel_most_fre']\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "encoder = OrdinalEncoder()\n",
    "for i in cat_features:\n",
    "    encoder.fit(merge_train[i].values.reshape(-1,1))\n",
    "    merge_train[i]=encoder.transform(merge_train[i].values.reshape(-1,1))\n",
    "    merge_val[i] = encoder.transform(merge_val[i].values.reshape(-1,1))\n",
    "\n",
    "data_path = 'guohuan-li' + '/new_data/' + 'eval' + '_'\n",
    "merge_val.to_csv(root_folder + data_path + \"final_merge.csv\",header=True,index=None)\n",
    "\n",
    "data_path = 'guohuan-li' + '/new_data/' + 'dev' + '_'\n",
    "merge_train.to_csv(root_folder + data_path + \"final_merge.csv\",header=True,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.8 + 0.5 * 1.5 -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2 = 0.55 * (-1.5)\n",
    "h3 = 0.55 *(2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47499999999999964"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2 = h2 -1\n",
    "h2 * 2 + h3 * 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
